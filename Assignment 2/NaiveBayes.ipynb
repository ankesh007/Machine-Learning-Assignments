{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizedInput(inputFileName,tokenize=False):\n",
    "    cleaned_list=[]\n",
    "    with open(inputFileName) as f:\n",
    "        docs = f.readlines()\n",
    "\n",
    "    for doc in docs:\n",
    "        raw=None\n",
    "        if(tokenize==True):\n",
    "            raw = tokenizer.tokenize(doc)\n",
    "        else:\n",
    "            raw=doc\n",
    "        cleaned_list.append(raw)\n",
    "    \n",
    "    return cleaned_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train=tokenizedInput('Dataset/Clean1/train_text.txt',tokenize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(x_train))\n",
    "# print(len(x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-281-264cd71cee25>, line 102)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-281-264cd71cee25>\"\u001b[0;36m, line \u001b[0;32m102\u001b[0m\n\u001b[0;31m    print(\"%4d\",%(confusion[i][j]),end=' ')\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self,x,y,laplace_smoother=1):\n",
    "        self.x=list(x)\n",
    "        self.y=list(y)\n",
    "        self.laplace_smoother=laplace_smoother\n",
    "        self.distinct_x=0\n",
    "        self.distinct_y=0\n",
    "        self.instances=len(y)\n",
    "        self.x_dict={}\n",
    "        self.vocabulary={}\n",
    "        self.y_dict={}\n",
    "        self.revMapy={}\n",
    "        self.wordsInClass=[]\n",
    "        self.instancesInClass=[]\n",
    "    \n",
    "    def calculateParameters(self):\n",
    "#         classes=np.unique(self.npy)\n",
    "        counter=0\n",
    "        for i in range(self.instances):\n",
    "            feature_vector=self.x[i]\n",
    "            feature_class=self.y[i]\n",
    "            mapped_class=counter\n",
    "            \n",
    "            if(feature_class in self.y_dict):\n",
    "                mapped_class=self.y_dict[feature_class]\n",
    "            else:\n",
    "                self.revMapy[counter]=feature_class\n",
    "                self.y_dict[feature_class]=counter\n",
    "                self.wordsInClass.append(0)\n",
    "                self.instancesInClass.append(0)\n",
    "                counter+=1\n",
    "            \n",
    "            self.instancesInClass[mapped_class]+=1\n",
    "            \n",
    "            for words in feature_vector:\n",
    "                if(words in self.vocabulary):\n",
    "                    pass\n",
    "                else:\n",
    "                    self.vocabulary[words]=1\n",
    "                \n",
    "                key=(words,mapped_class)\n",
    "                self.wordsInClass[mapped_class]+=1\n",
    "                \n",
    "                if(key in self.x_dict):\n",
    "                    self.x_dict[key]+=1\n",
    "                else:\n",
    "                    self.x_dict[key]=1\n",
    "            \n",
    "        self.distinct_x=len(self.vocabulary)\n",
    "        self.distinct_y=len(self.y_dict)\n",
    "#         self.printParameters()\n",
    "    \n",
    "    def getLogPrior(self,label):\n",
    "        return math.log(float(self.instancesInClass[label])/self.instances)\n",
    "    \n",
    "    def getLogProb(self,attribute,label):\n",
    "        \n",
    "        occurences=0\n",
    "        key=(attribute,label)\n",
    "        \n",
    "        if key in self.x_dict:\n",
    "            occurences=self.x_dict[key]\n",
    "        \n",
    "        occurences+=self.laplace_smoother\n",
    "        \n",
    "        total_occurences_in_class=self.wordsInClass[label]\n",
    "        total_occurences_in_class+=self.distinct_x*self.laplace_smoother\n",
    "        \n",
    "        return math.log(float(occurences)/total_occurences_in_class)\n",
    "        \n",
    "    def getClass(self,x):\n",
    "        max_log_prob=-1e9\n",
    "        label=-1\n",
    "        \n",
    "        for i in range(self.distinct_y):\n",
    "            log_prob_x_given_y=0\n",
    "            \n",
    "            for attributes in x:\n",
    "                log_prob_x_given_y+=self.getLogProb(attributes,i)\n",
    "            \n",
    "            log_prob_x=log_prob_x_given_y+self.getLogPrior(i)\n",
    "            \n",
    "            if(log_prob_x>max_log_prob):\n",
    "                max_log_prob=log_prob_x\n",
    "                label=self.revMapy[i]\n",
    "            \n",
    "        return label\n",
    "    \n",
    "    def ConfusionMatrix(self,y,predy):\n",
    "#         confusion=[[0]*self.distinct_y]*self.distinct_y\n",
    "        confusion = [ [0]*self.distinct_y for _ in range(self.distinct_y) ]\n",
    "        \n",
    "        tests=len(y)\n",
    "        \n",
    "        for i in range(tests):\n",
    "            confusion[self.y_dict[predy[i]]][self.y_dict[y[i]]]+=1\n",
    "        \n",
    "        for i in range(self.distinct_y):\n",
    "            for j in range(self.distinct_y):\n",
    "#                 print((i,j),end=' ')                                \n",
    "                print(\"%4d\",%(confusion[i][j]),end=' ')\n",
    "            print()\n",
    "        \n",
    "        \n",
    "    def getAccuracy(self,x,y,printConfusionMatrix=False):\n",
    "        total_tests=len(y)\n",
    "        passed_tests=0\n",
    "        prediction_list=[]\n",
    "        \n",
    "        for i in range(total_tests):\n",
    "            xi=x[i]\n",
    "            yi=y[i]\n",
    "            \n",
    "#             if y[i] in self.y_dict:\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 continue\n",
    "            pred_yi=self.getClass(xi)\n",
    "            prediction_list.append(pred_yi)\n",
    "            if(pred_yi==yi):\n",
    "                passed_tests+=1\n",
    "        \n",
    "        if(printConfusionMatrix==True):\n",
    "            self.ConfusionMatrix(y,pred_yi)\n",
    "                \n",
    "        return [prediction_list,(float(passed_tests))/total_tests]\n",
    "    \n",
    "    def getAccuracyRandomPredict(self,x,y):\n",
    "        \n",
    "        average_over=10\n",
    "        total_accuracy=0\n",
    "        \n",
    "        for i in range(average_over):\n",
    "            \n",
    "            total_tests=len(y)\n",
    "            passed_tests=0\n",
    "\n",
    "            for i in range(total_tests):\n",
    "                yi=y[i]\n",
    "\n",
    "                pred_yi=random.randint(0,self.distinct_y-1)\n",
    "                pred_yi=self.revMapy[pred_yi]\n",
    "\n",
    "                if(pred_yi==yi):\n",
    "                    passed_tests+=1\n",
    "\n",
    "            total_accuracy+=(float(passed_tests))/total_tests\n",
    "        \n",
    "        return total_accuracy/average_over\n",
    "    \n",
    "    def getAccuracyMajorityPredictor(self,x,y):\n",
    "        max_occ=-1\n",
    "        majority_class=-1\n",
    "        total_tests=len(y)\n",
    "        passed_tests=0        \n",
    "        \n",
    "        for i in range(self.distinct_y):\n",
    "            if(max_occ<self.instancesInClass[i]):\n",
    "                max_occ=self.instancesInClass[i]\n",
    "                majority_class=i\n",
    "        \n",
    "        if(majority_class==-1):\n",
    "            return 0\n",
    "        \n",
    "        majority_class=self.revMapy[majority_class]\n",
    "        \n",
    "        for yi in y:\n",
    "            if(yi==majority_class):\n",
    "                passed_tests+=1\n",
    "                \n",
    "        return (float(passed_tests))/total_tests       \n",
    "\n",
    "    def printParameters(self):\n",
    "        print(\"Vocabulary Size:\",self.distinct_x)\n",
    "        print(\"Classes:\",self.distinct_y)\n",
    "        print(\"X_Y's:\",len(self.x_dict))\n",
    "        \n",
    "        print(\"Mapped Classes:\", self.y_dict)\n",
    "        \n",
    "        print(\"Instances In Mapped class:\",end='')\n",
    "        \n",
    "        for counts in self.instancesInClass:\n",
    "            print(counts,end=' ')\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"Words In Mapped class:\",end='')\n",
    "        \n",
    "        for counts in self.wordsInClass:\n",
    "            print(counts,end=' ')\n",
    "        \n",
    "        print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=tokenizedInput('Dataset/Clean1/train_text.txt',tokenize=True)\n",
    "y_train=tokenizedInput('Dataset/Clean1/imdb_train_labels.txt',tokenize=False)\n",
    "y_train=list(map(int,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaiveBayesClassifier=NaiveBayes(x=x_train,y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaiveBayesClassifier.calculateParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5956618\n"
     ]
    }
   ],
   "source": [
    "print (sum(NaiveBayesClassifier.wordsInClass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (NaiveBayesClassifier.getAccuracy(x_train,y_train))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=tokenizedInput('Dataset/Clean1/test_text.txt',tokenize=True)\n",
    "y_test=tokenizedInput('Dataset/Clean1/imdb_test_labels.txt',tokenize=False)\n",
    "y_test=list(map(int,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "[a,b]=(NaiveBayesClassifier.getAccuracy(x_test,y_test,printConfusionMatrix=False))\n",
    "# print (b)\n",
    "# print(NaiveBayesClassifier.ConfusionMatrix(a,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (NaiveBayesClassifier.getAccuracyRandomPredict(x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (NaiveBayesClassifier.getAccuracyMajorityPredictor(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38468"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%4d 3127 %4d 1023 %4d 537 %4d 1141 %4d 142 %4d 187 %4d 94 %4d 134 \n",
      "%4d 558 %4d 715 %4d 517 %4d 466 %4d 119 %4d 227 %4d 54 %4d 63 \n",
      "%4d 180 %4d 317 %4d 427 %4d 155 %4d 126 %4d 222 %4d 55 %4d 36 \n",
      "%4d 179 %4d 131 %4d 74 %4d 124 %4d 14 %4d 28 %4d 5 %4d 18 \n",
      "%4d 45 %4d 63 %4d 80 %4d 23 %4d 233 %4d 218 %4d 187 %4d 155 \n",
      "%4d 102 %4d 166 %4d 263 %4d 96 %4d 489 %4d 674 %4d 273 %4d 260 \n",
      "%4d 12 %4d 13 %4d 9 %4d 6 %4d 58 %4d 47 %4d 51 %4d 90 \n",
      "%4d 796 %4d 422 %4d 400 %4d 333 %4d 1360 %4d 1032 %4d 1583 %4d 4266 \n"
     ]
    }
   ],
   "source": [
    "NaiveBayesClassifier.ConfusionMatrix(y_test,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 7, 2: 6, 3: 4, 4: 5, 7: 2, 8: 1, 9: 3, 10: 0}"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NaiveBayesClassifier.y_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
