{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpd=pd.read_csv(\"Dataset/mnist/train.csv\",header=None,sep=',')\n",
    "testpd=pd.read_csv(\"Dataset/mnist/test.csv\",header=None,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    \n",
    "    def __init__(self,x,y,normalize_x=True):\n",
    "        self.x=np.copy(x).astype(float)\n",
    "        self.y=np.copy(y)\n",
    "        [instances,dimensions]=x.shape\n",
    "        self.dimensions=dimensions\n",
    "        self.instances=instances\n",
    "        self.normalize_param=np.ones((1,dimensions))\n",
    "        self.normalize_x=normalize_x\n",
    "        label=np.unique(y).shape[0]\n",
    "        self.label=label\n",
    "        self.weight=np.zeros((self.label,self.label,dimensions))\n",
    "        self.bias=np.zeros((self.label,self.label))\n",
    "        self.epoch=np.zeros((self.label,self.label))\n",
    "        self.train_steps=np.zeros((self.label,self.label))\n",
    "        self.isTrained=np.zeros((self.label,self.label))\n",
    "\n",
    "        if(normalize_x==True):\n",
    "            self.normalize_param*=255\n",
    "            self.x=self.normalize(self.x)\n",
    "        \n",
    "    def normalize(self,x):\n",
    "        return (x/self.normalize_param)\n",
    "    \n",
    "    def resetParam(self,i,j):\n",
    "        self.weight[i,j,:]*=0\n",
    "        self.bias[i,j]=0\n",
    "        self.epoch[i,j]=0\n",
    "        self.train_steps[i,j]=0\n",
    "        self.isTrained[i,j]=0\n",
    "        return\n",
    "    \n",
    "    def evaluate(self,x,y,weight,bias):\n",
    "        return (np.matmul(x,weight)+bias)*y\n",
    "    \n",
    "    def getLoss(self,x,y,lamda,C,weight,bias):\n",
    "        loss=np.matmul(np.transpose(weight),weight)*lamda\n",
    "        evaluate=1-self.evaluate(x=x,y=y,weight=weight,bias=bias)\n",
    "        mask=(evaluate>0).astype(int)\n",
    "        evaluate*=mask\n",
    "        loss+=C*np.sum(evaluate)\n",
    "        return loss\n",
    "    \n",
    "    def getWeight(self,i,j):\n",
    "        return self.weight[i,j,:][:,np.newaxis]\n",
    "    \n",
    "    def train1Classier(self,weight,bias,train_steps,epoch,x=None,y=None,lamda=1,C=1,max_steps=4000,batch_mode=True,batch_size=100,epsilon=0.001,log_every=1000):\n",
    "\n",
    "        if(x is None):\n",
    "            x=self.x\n",
    "        if(y is None):\n",
    "            y=self.y\n",
    "            \n",
    "        xy=-(x*y)\n",
    "        [instances,dim]=x.shape\n",
    "        [instances,ydim]=y.shape\n",
    "        \n",
    "        if(batch_mode==False):\n",
    "            batch_size=instances\n",
    "        \n",
    "        prev_loss=self.getLoss(x=x,y=y,lamda=lamda,C=C,weight=weight,bias=bias)\n",
    "            \n",
    "        Trained=False    \n",
    "        cur_steps=0\n",
    "        while(Trained==False and cur_steps<max_steps):\n",
    "            cur_instance=0\n",
    "            epoch+=1\n",
    "            \n",
    "            while(cur_instance<instances and cur_steps<max_steps):\n",
    "                train_steps+=1\n",
    "                cur_steps+=1\n",
    "                eta=1.0/(cur_steps)\n",
    "                up_lim=min(instances,cur_instance+batch_size)\n",
    "                batch_x=x[cur_instance:up_lim,0:dim]\n",
    "                batch_y=y[cur_instance:up_lim,0:ydim]\n",
    "                batch_xy=xy[cur_instance:up_lim,0:dim]\n",
    "                examples=up_lim-cur_instance\n",
    "                cur_instance=up_lim\n",
    "                evaluate=self.evaluate(batch_x,batch_y,weight,bias)\n",
    "                mask=((1-evaluate)>=0).astype(int)\n",
    "                weight=(1-eta*lamda)*weight - (eta*C*(np.sum(batch_xy*mask,axis=0)[:,np.newaxis]))\n",
    "                bias=bias+((np.sum(mask*batch_y))*C*eta)\n",
    "                \n",
    "                if(train_steps%log_every==0):\n",
    "                    print(train_steps)\n",
    "#                     print(\"Batch Loss at Steps=\",train_steps,\" is \",self.getLoss(x,y,lamda,C,weight,bias))\n",
    "            \n",
    "            cur_loss=self.getLoss(x,y,lamda,C,weight,bias)\n",
    "            if(abs(cur_loss-prev_loss)<epsilon):\n",
    "                prev_loss=cur_loss\n",
    "                Trained=True\n",
    "                \n",
    "            prev_loss=cur_loss\n",
    "        \n",
    "        print(\"Loss on Ending:\",prev_loss)\n",
    "\n",
    "        return (weight,bias,epoch,train_steps,Trained)\n",
    "    \n",
    "    def trainIJclassifier(self,i,j,reset=False):\n",
    "        \n",
    "        if(reset==True):\n",
    "            self.resetParam(i,j)\n",
    "        \n",
    "        print(\"********************************************************\")\n",
    "        print(i,\" \",j,\" \",\"Classifier\")\n",
    "        \n",
    "        if(self.isTrained[i,j]==1):\n",
    "            return\n",
    "        \n",
    "        aux_arr=(self.y==i).astype(int)+(self.y==j).astype(int)\n",
    "\n",
    "        newx=self.x[aux_arr[:,0]>0]\n",
    "        newy=self.y[aux_arr[:,0]>0]\n",
    "        newy2=np.copy(newy)\n",
    "        newy=((newy2==i).astype(int))-((newy2==j).astype(int))\n",
    "\n",
    "        (weight,bias,epoch,train_steps,Trained)=self.train1Classier(weight=self.getWeight(i,j),\n",
    "                                          bias=self.bias[i,j],\n",
    "                                          x=newx,\n",
    "                                          y=newy,\n",
    "                                          train_steps=self.train_steps[i,j],\n",
    "                                          epoch=self.epoch[i,j])\n",
    "        \n",
    "        self.weight[i,j,:]=weight[:,0]\n",
    "        self.bias[i,j]=bias\n",
    "        self.epoch[i,j]=epoch\n",
    "        self.train_steps[i,j]=train_steps\n",
    "        \n",
    "        if(Trained==True):\n",
    "            self.isTrained[i,j]=1\n",
    "        \n",
    "    def train(self):\n",
    "        for i in range(self.label):\n",
    "            for j in range(i+1,self.label,1):\n",
    "                self.trainIJclassifier(i,j)\n",
    "\n",
    "    def getClass(self,x,i,j):\n",
    "        weight=self.getWeight(i,j)\n",
    "        \n",
    "        if(np.matmul(weight.T,x)+self.bias[i,j]>0):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "                \n",
    "    def predictInstance(self,x):\n",
    "        \n",
    "        count=[]\n",
    "        for i in range(self.label):\n",
    "            count.append(0)\n",
    "            \n",
    "        for i in range(self.label):\n",
    "            for j in range(i+1,self.label,1):\n",
    "                aux_class=self.getClass(x,i,j)\n",
    "                if(aux_class==1):\n",
    "                    count[i]+=1\n",
    "                else:\n",
    "                    count[j]+=1\n",
    "        \n",
    "        coun=-1\n",
    "        lab=-1\n",
    "        \n",
    "        for i in range(self.label):\n",
    "\n",
    "            if(coun<=count[i]):\n",
    "                coun=count[i]\n",
    "                lab=i\n",
    "        return lab\n",
    "    \n",
    "    def predict(self,x):\n",
    "        predictions=[]\n",
    "        \n",
    "        instances=x.shape[0]\n",
    "        \n",
    "        for i in range(instances):\n",
    "            predictions.append(self.predictInstance(x[i][::,np.newaxis]))\n",
    "        \n",
    "        return np.asarray(predictions)[:,np.newaxis]\n",
    "            \n",
    "    def getAccuracy(self,x,y):\n",
    "        predictions=self.predict(x.astype(float)/self.normalize_param)\n",
    "        \n",
    "        mask_correct=np.sum((predictions==y).astype(int))\n",
    "        instances=y.shape[0]\n",
    "        \n",
    "        return float(mask_correct)/instances\n",
    "    \n",
    "    def saveModel(self,filename):\n",
    "        file_handler=open(filename,\"w\")\n",
    "        pickle.dump(self,file_handler)\n",
    "        f.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDataframe(dataframe):\n",
    "    values=dataframe.values\n",
    "    [instances,dim]=values.shape\n",
    "    x=values[:,0:dim-1]\n",
    "    y=values[:,dim-1:dim]\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train)=parseDataframe(trainpd)\n",
    "(x_test,y_test)=parseDataframe(testpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 784)\n",
      "(20000, 1)\n",
      "(10000, 784)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_instance=SVM(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************\n",
      "0   1   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[14.97453016]]\n",
      "********************************************************\n",
      "0   2   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[88.52703531]]\n",
      "********************************************************\n",
      "0   3   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[55.77293465]]\n",
      "********************************************************\n",
      "0   4   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "Loss on Ending: [[57.4733256]]\n",
      "********************************************************\n",
      "0   5   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[160.53112564]]\n",
      "********************************************************\n",
      "0   6   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[76.76697175]]\n",
      "********************************************************\n",
      "0   7   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[59.35608593]]\n",
      "********************************************************\n",
      "0   8   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "Loss on Ending: [[67.88227316]]\n",
      "********************************************************\n",
      "0   9   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[96.78561173]]\n",
      "********************************************************\n",
      "1   2   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[106.38987692]]\n",
      "********************************************************\n",
      "1   3   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[86.13256116]]\n",
      "********************************************************\n",
      "1   4   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "Loss on Ending: [[38.69698552]]\n",
      "********************************************************\n",
      "1   5   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "Loss on Ending: [[60.75783069]]\n",
      "********************************************************\n",
      "1   6   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[23.66966436]]\n",
      "********************************************************\n",
      "1   7   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[52.49676537]]\n",
      "********************************************************\n",
      "1   8   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[258.95391161]]\n",
      "********************************************************\n",
      "1   9   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[40.28140082]]\n",
      "********************************************************\n",
      "2   3   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[229.34870964]]\n",
      "********************************************************\n",
      "2   4   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[143.18636167]]\n",
      "********************************************************\n",
      "2   5   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[195.9920591]]\n",
      "********************************************************\n",
      "2   6   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "Loss on Ending: [[183.65271192]]\n",
      "********************************************************\n",
      "2   7   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[139.01563414]]\n",
      "********************************************************\n",
      "2   8   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[250.0858535]]\n",
      "********************************************************\n",
      "2   9   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[128.68859734]]\n",
      "********************************************************\n",
      "3   4   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[78.38540318]]\n",
      "********************************************************\n",
      "3   5   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[424.15155001]]\n",
      "********************************************************\n",
      "3   6   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[37.46881583]]\n",
      "********************************************************\n",
      "3   7   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[147.75785314]]\n",
      "********************************************************\n",
      "3   8   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[310.48998479]]\n",
      "********************************************************\n",
      "3   9   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[204.06617246]]\n",
      "********************************************************\n",
      "4   5   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[72.99727422]]\n",
      "********************************************************\n",
      "4   6   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[87.32203102]]\n",
      "********************************************************\n",
      "4   7   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[118.23001736]]\n",
      "********************************************************\n",
      "4   8   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[166.45646886]]\n",
      "********************************************************\n",
      "4   9   Classifier\n",
      "Loss on Ending: [[356.83005507]]\n",
      "********************************************************\n",
      "5   6   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[202.39632885]]\n",
      "********************************************************\n",
      "5   7   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[65.31618565]]\n",
      "********************************************************\n",
      "5   8   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[490.39001745]]\n",
      "********************************************************\n",
      "5   9   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[125.92048918]]\n",
      "********************************************************\n",
      "6   7   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "Loss on Ending: [[22.4543073]]\n",
      "********************************************************\n",
      "6   8   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[102.7274294]]\n",
      "********************************************************\n",
      "6   9   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[58.0157586]]\n",
      "********************************************************\n",
      "7   8   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[131.08985338]]\n",
      "********************************************************\n",
      "7   9   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[397.0498762]]\n",
      "********************************************************\n",
      "8   9   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[195.78830225]]\n"
     ]
    }
   ],
   "source": [
    "SVM_instance.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9254"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_instance.getAccuracy(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"SVM_Model\",\"wb\")\n",
    "pickle.dump(SVM_instance,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=np.copy(SVM_instance.weight)\n",
    "bias=np.copy(SVM_instance.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_instance.weight=np.copy(weights)\n",
    "SVM_bias=np.copy(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"SVM_Model\", \"rb\") as f:\n",
    "    SVM_instance = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
