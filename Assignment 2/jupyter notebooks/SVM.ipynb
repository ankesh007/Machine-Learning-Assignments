{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'Dataset/mnist/train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-211000337664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainpd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset/mnist/train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtestpd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset/mnist/test.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ankesh/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ankesh/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ankesh/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ankesh/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ankesh/.local/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'Dataset/mnist/train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "trainpd=pd.read_csv(\"Dataset/mnist/train.csv\",header=None,sep=',')\n",
    "testpd=pd.read_csv(\"Dataset/mnist/test.csv\",header=None,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    \n",
    "    def __init__(self,x,y,normalize_x=True):\n",
    "        self.x=np.copy(x).astype(float)\n",
    "        self.y=np.copy(y)\n",
    "        [instances,dimensions]=x.shape\n",
    "        self.dimensions=dimensions\n",
    "        self.instances=instances\n",
    "        self.normalize_param=np.ones((1,dimensions))\n",
    "        self.normalize_x=normalize_x\n",
    "        label=np.unique(y).shape[0]\n",
    "        self.label=label\n",
    "        self.weight=np.zeros((self.label,self.label,dimensions))\n",
    "        self.bias=np.zeros((self.label,self.label))\n",
    "        self.epoch=np.zeros((self.label,self.label))\n",
    "        self.train_steps=np.zeros((self.label,self.label))\n",
    "        self.isTrained=np.zeros((self.label,self.label))\n",
    "\n",
    "        if(normalize_x==True):\n",
    "            self.normalize_param*=255\n",
    "            self.x=self.normalize(self.x)\n",
    "        \n",
    "    def normalize(self,x):\n",
    "        return (x/self.normalize_param)\n",
    "    \n",
    "    def resetParam(self,i,j):\n",
    "        self.weight[i,j,:]*=0\n",
    "        self.bias[i,j]=0\n",
    "        self.epoch[i,j]=0\n",
    "        self.train_steps[i,j]=0\n",
    "        self.isTrained[i,j]=0\n",
    "        return\n",
    "    \n",
    "    def evaluate(self,x,y,weight,bias):\n",
    "        return (np.matmul(x,weight)+bias)*y\n",
    "    \n",
    "    def getLoss(self,x,y,lamda,C,weight,bias):\n",
    "        loss=np.matmul(np.transpose(weight),weight)*lamda\n",
    "        evaluate=1-self.evaluate(x=x,y=y,weight=weight,bias=bias)\n",
    "        mask=(evaluate>0).astype(int)\n",
    "        evaluate*=mask\n",
    "        loss+=C*np.sum(evaluate)\n",
    "        return loss\n",
    "    \n",
    "    def getWeight(self,i,j):\n",
    "        return self.weight[i,j,:][:,np.newaxis]\n",
    "    \n",
    "    def train1Classier(self,weight,bias,train_steps,epoch,x=None,y=None,lamda=1,C=1,max_steps=4000,batch_mode=True,batch_size=100,epsilon=0.001,log_every=1000):\n",
    "\n",
    "        if(x is None):\n",
    "            x=self.x\n",
    "        if(y is None):\n",
    "            y=self.y\n",
    "            \n",
    "        xy=-(x*y)\n",
    "        [instances,dim]=x.shape\n",
    "        [instances,ydim]=y.shape\n",
    "        \n",
    "        if(batch_mode==False):\n",
    "            batch_size=instances\n",
    "        \n",
    "        prev_loss=self.getLoss(x=x,y=y,lamda=lamda,C=C,weight=weight,bias=bias)\n",
    "            \n",
    "        Trained=False    \n",
    "        cur_steps=0\n",
    "        while(Trained==False and cur_steps<max_steps):\n",
    "            cur_instance=0\n",
    "            epoch+=1\n",
    "            \n",
    "            while(cur_instance<instances and cur_steps<max_steps):\n",
    "                train_steps+=1\n",
    "                cur_steps+=1\n",
    "                eta=1.0/(cur_steps)\n",
    "                up_lim=min(instances,cur_instance+batch_size)\n",
    "                batch_x=x[cur_instance:up_lim,0:dim]\n",
    "                batch_y=y[cur_instance:up_lim,0:ydim]\n",
    "                batch_xy=xy[cur_instance:up_lim,0:dim]\n",
    "                examples=up_lim-cur_instance\n",
    "                cur_instance=up_lim\n",
    "                evaluate=self.evaluate(batch_x,batch_y,weight,bias)\n",
    "                mask=((1-evaluate)>=0).astype(int)\n",
    "                weight=(1-eta*lamda)*weight - (eta*C*(np.sum(batch_xy*mask,axis=0)[:,np.newaxis]))\n",
    "                bias=bias+((np.sum(mask*batch_y))*C*eta)\n",
    "                \n",
    "                if(train_steps%log_every==0):\n",
    "                    print(train_steps)\n",
    "#                     print(\"Batch Loss at Steps=\",train_steps,\" is \",self.getLoss(x,y,lamda,C,weight,bias))\n",
    "            \n",
    "            cur_loss=self.getLoss(x,y,lamda,C,weight,bias)\n",
    "            if(abs(cur_loss-prev_loss)<epsilon):\n",
    "                prev_loss=cur_loss\n",
    "                Trained=True\n",
    "                \n",
    "            prev_loss=cur_loss\n",
    "        \n",
    "        print(\"Loss on Ending:\",prev_loss)\n",
    "\n",
    "        return (weight,bias,epoch,train_steps,Trained)\n",
    "    \n",
    "    def trainIJclassifier(self,i,j,reset=False):\n",
    "        \n",
    "        if(reset==True):\n",
    "            self.resetParam(i,j)\n",
    "        \n",
    "        print(\"********************************************************\")\n",
    "        print(i,\" \",j,\" \",\"Classifier\")\n",
    "        \n",
    "        if(self.isTrained[i,j]==1):\n",
    "            return\n",
    "        \n",
    "        aux_arr=(self.y==i).astype(int)+(self.y==j).astype(int)\n",
    "\n",
    "        newx=self.x[aux_arr[:,0]>0]\n",
    "        newy=self.y[aux_arr[:,0]>0]\n",
    "        newy2=np.copy(newy)\n",
    "        newy=((newy2==i).astype(int))-((newy2==j).astype(int))\n",
    "\n",
    "        (weight,bias,epoch,train_steps,Trained)=self.train1Classier(weight=self.getWeight(i,j),\n",
    "                                          bias=self.bias[i,j],\n",
    "                                          x=newx,\n",
    "                                          y=newy,\n",
    "                                          train_steps=self.train_steps[i,j],\n",
    "                                          epoch=self.epoch[i,j])\n",
    "        \n",
    "        self.weight[i,j,:]=weight[:,0]\n",
    "        self.bias[i,j]=bias\n",
    "        self.epoch[i,j]=epoch\n",
    "        self.train_steps[i,j]=train_steps\n",
    "        \n",
    "        if(Trained==True):\n",
    "            self.isTrained[i,j]=1\n",
    "        \n",
    "    def train(self):\n",
    "        for i in range(self.label):\n",
    "            for j in range(i+1,self.label,1):\n",
    "                self.trainIJclassifier(i,j)\n",
    "\n",
    "    def getClass(self,x,i,j):\n",
    "        weight=self.getWeight(i,j)\n",
    "        \n",
    "        if(np.matmul(weight.T,x)+self.bias[i,j]>0):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "                \n",
    "    def predictInstance(self,x):\n",
    "        \n",
    "        count=[]\n",
    "        for i in range(self.label):\n",
    "            count.append(0)\n",
    "            \n",
    "        for i in range(self.label):\n",
    "            for j in range(i+1,self.label,1):\n",
    "                aux_class=self.getClass(x,i,j)\n",
    "                if(aux_class==1):\n",
    "                    count[i]+=1\n",
    "                else:\n",
    "                    count[j]+=1\n",
    "        \n",
    "        coun=-1\n",
    "        lab=-1\n",
    "        \n",
    "        for i in range(self.label):\n",
    "\n",
    "            if(coun<=count[i]):\n",
    "                coun=count[i]\n",
    "                lab=i\n",
    "        return lab\n",
    "    \n",
    "    def predict(self,x):\n",
    "        predictions=[]\n",
    "        \n",
    "        instances=x.shape[0]\n",
    "        \n",
    "        for i in range(instances):\n",
    "            predictions.append(self.predictInstance(x[i][::,np.newaxis]))\n",
    "        \n",
    "        return np.asarray(predictions)[:,np.newaxis]\n",
    "            \n",
    "    def getAccuracy(self,x,y):\n",
    "        predictions=self.predict(x.astype(float)/self.normalize_param)\n",
    "        \n",
    "        mask_correct=np.sum((predictions==y).astype(int))\n",
    "        instances=y.shape[0]\n",
    "        \n",
    "        return float(mask_correct)/instances\n",
    "    \n",
    "    def saveModel(self,filename):\n",
    "        file_handler=open(filename,\"w\")\n",
    "        pickle.dump(self,file_handler)\n",
    "        f.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDataframe(dataframe):\n",
    "    values=dataframe.values\n",
    "    [instances,dim]=values.shape\n",
    "    x=values[:,0:dim-1]\n",
    "    y=values[:,dim-1:dim]\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainpd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4101cffb8781>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparseDataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainpd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparseDataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestpd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainpd' is not defined"
     ]
    }
   ],
   "source": [
    "(x_train,y_train)=parseDataframe(trainpd)\n",
    "(x_test,y_test)=parseDataframe(testpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 784)\n",
      "(20000, 1)\n",
      "(10000, 784)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_instance=SVM(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************\n",
      "0   1   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[14.97453016]]\n",
      "********************************************************\n",
      "0   2   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[88.52703531]]\n",
      "********************************************************\n",
      "0   3   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[55.77293465]]\n",
      "********************************************************\n",
      "0   4   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "Loss on Ending: [[57.4733256]]\n",
      "********************************************************\n",
      "0   5   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[160.53112564]]\n",
      "********************************************************\n",
      "0   6   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[76.76697175]]\n",
      "********************************************************\n",
      "0   7   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[59.35608593]]\n",
      "********************************************************\n",
      "0   8   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "Loss on Ending: [[67.88227316]]\n",
      "********************************************************\n",
      "0   9   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[96.78561173]]\n",
      "********************************************************\n",
      "1   2   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[106.38987692]]\n",
      "********************************************************\n",
      "1   3   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[86.13256116]]\n",
      "********************************************************\n",
      "1   4   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "Loss on Ending: [[38.69698552]]\n",
      "********************************************************\n",
      "1   5   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "Loss on Ending: [[60.75783069]]\n",
      "********************************************************\n",
      "1   6   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[23.66966436]]\n",
      "********************************************************\n",
      "1   7   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[52.49676537]]\n",
      "********************************************************\n",
      "1   8   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[258.95391161]]\n",
      "********************************************************\n",
      "1   9   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[40.28140082]]\n",
      "********************************************************\n",
      "2   3   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[229.34870964]]\n",
      "********************************************************\n",
      "2   4   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[143.18636167]]\n",
      "********************************************************\n",
      "2   5   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[195.9920591]]\n",
      "********************************************************\n",
      "2   6   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "Loss on Ending: [[183.65271192]]\n",
      "********************************************************\n",
      "2   7   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[139.01563414]]\n",
      "********************************************************\n",
      "2   8   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[250.0858535]]\n",
      "********************************************************\n",
      "2   9   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[128.68859734]]\n",
      "********************************************************\n",
      "3   4   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[78.38540318]]\n",
      "********************************************************\n",
      "3   5   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[424.15155001]]\n",
      "********************************************************\n",
      "3   6   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[37.46881583]]\n",
      "********************************************************\n",
      "3   7   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[147.75785314]]\n",
      "********************************************************\n",
      "3   8   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[310.48998479]]\n",
      "********************************************************\n",
      "3   9   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[204.06617246]]\n",
      "********************************************************\n",
      "4   5   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[72.99727422]]\n",
      "********************************************************\n",
      "4   6   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[87.32203102]]\n",
      "********************************************************\n",
      "4   7   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[118.23001736]]\n",
      "********************************************************\n",
      "4   8   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[166.45646886]]\n",
      "********************************************************\n",
      "4   9   Classifier\n",
      "Loss on Ending: [[356.83005507]]\n",
      "********************************************************\n",
      "5   6   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[202.39632885]]\n",
      "********************************************************\n",
      "5   7   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[65.31618565]]\n",
      "********************************************************\n",
      "5   8   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[490.39001745]]\n",
      "********************************************************\n",
      "5   9   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[125.92048918]]\n",
      "********************************************************\n",
      "6   7   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "Loss on Ending: [[22.4543073]]\n",
      "********************************************************\n",
      "6   8   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[102.7274294]]\n",
      "********************************************************\n",
      "6   9   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[58.0157586]]\n",
      "********************************************************\n",
      "7   8   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[131.08985338]]\n",
      "********************************************************\n",
      "7   9   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[397.0498762]]\n",
      "********************************************************\n",
      "8   9   Classifier\n",
      "1000.0\n",
      "2000.0\n",
      "3000.0\n",
      "4000.0\n",
      "Loss on Ending: [[195.78830225]]\n"
     ]
    }
   ],
   "source": [
    "SVM_instance.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9254"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_instance.getAccuracy(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"SVM_Model\",\"wb\")\n",
    "pickle.dump(SVM_instance,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=np.copy(SVM_instance.weight)\n",
    "bias=np.copy(SVM_instance.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_instance.weight=np.copy(weights)\n",
    "SVM_bias=np.copy(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../model_SVM/SVM_my_model\", \"rb\") as f:\n",
    "    SVM_instance = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_instance.bias.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
