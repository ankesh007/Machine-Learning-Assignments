{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizedInput(inputFileName,tokenize=True):\n",
    "    cleaned_list=[]\n",
    "    with open(inputFileName) as f:\n",
    "        docs = f.readlines()\n",
    "\n",
    "    for doc in docs:\n",
    "        raw=None\n",
    "        if(tokenize==True):\n",
    "            raw = tokenizer.tokenize(doc)\n",
    "        else:\n",
    "            raw=doc\n",
    "        cleaned_list.append(raw)\n",
    "    \n",
    "    return cleaned_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizedInput_WithNGrams(inputFileName,grams=1):\n",
    "    cleaned_list=[]\n",
    "    with open(inputFileName) as f:\n",
    "        docs = f.readlines()\n",
    "\n",
    "    for doc in docs:\n",
    "        raw2=[]\n",
    "        raw = tokenizer.tokenize(doc)\n",
    "        leng=len(raw)\n",
    "        for k in range(grams-1,leng,1):\n",
    "            new_token=\"\"\n",
    "            for l in range(k-grams+1,k+1,1):\n",
    "                new_token+=\" \"+raw[l]\n",
    "            raw2.append(new_token)\n",
    "        cleaned_list.append(raw2+raw)\n",
    "    \n",
    "    return cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train=tokenizedInput_WithNGrams('Dataset/Stem/toy',grams=2)\n",
    "# y_train=tokenizedInput('Dataset/imdb/imdb_train_labels.txt',tokenize=False)\n",
    "# y_train=list(map(int,y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print ((x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self,x,y,laplace_smoother=1):\n",
    "        self.x=list(x)\n",
    "        self.y=list(y)\n",
    "        self.laplace_smoother=laplace_smoother\n",
    "        self.distinct_x=0\n",
    "        self.distinct_y=0\n",
    "        self.instances=len(y)\n",
    "        self.x_dict={}\n",
    "        self.vocabulary={}\n",
    "        self.y_dict={}\n",
    "        self.revMapy={}\n",
    "        self.wordsInClass=[]\n",
    "        self.instancesInClass=[]\n",
    "        self.weight_dict={}\n",
    "        self.classweight=[]\n",
    "    \n",
    "    def tfidf(self):\n",
    "        \n",
    "        for i in range(self.distinct_y):\n",
    "            self.classweight.append(0)\n",
    "        \n",
    "        for key,value in self.vocabulary.items():\n",
    "            \n",
    "            docs=0\n",
    "            for i in range(self.distinct_y):\n",
    "                if((key,i) in self.x_dict):\n",
    "                    docs+=1\n",
    "            \n",
    "            for i in range(self.distinct_y):\n",
    "                if((key,i) in self.x_dict):\n",
    "                    temp=self.x_dict[(key,i)]*(math.log(10/docs))\n",
    "                    self.weight_dict[(key,i)]=temp\n",
    "                    self.classweight[i]+=temp\n",
    "    \n",
    "    \n",
    "                \n",
    "    \n",
    "    def calculateParameters(self):\n",
    "#         classes=np.unique(self.npy)\n",
    "        counter=0\n",
    "        for i in range(self.instances):\n",
    "            feature_vector=self.x[i]\n",
    "            feature_class=self.y[i]\n",
    "            mapped_class=counter\n",
    "            \n",
    "            if(feature_class in self.y_dict):\n",
    "                mapped_class=self.y_dict[feature_class]\n",
    "            else:\n",
    "                self.revMapy[counter]=feature_class\n",
    "                self.y_dict[feature_class]=counter\n",
    "                self.wordsInClass.append(0)\n",
    "                self.instancesInClass.append(0)\n",
    "                counter+=1\n",
    "            \n",
    "            self.instancesInClass[mapped_class]+=1\n",
    "            \n",
    "            for words in feature_vector:\n",
    "                if(words in self.vocabulary):\n",
    "                    self.vocabulary[words]+=1\n",
    "                else:\n",
    "                    self.vocabulary[words]=1\n",
    "                \n",
    "                key=(words,mapped_class)\n",
    "                self.wordsInClass[mapped_class]+=1\n",
    "                \n",
    "                if(key in self.x_dict):\n",
    "                    self.x_dict[key]+=1\n",
    "                else:\n",
    "                    self.x_dict[key]=1\n",
    "            \n",
    "        self.distinct_x=len(self.vocabulary)\n",
    "        self.distinct_y=len(self.y_dict)\n",
    "#         self.printParameters()\n",
    "\n",
    "    def purge(self,threshold=7):\n",
    "        \n",
    "        emp_lis=[]\n",
    "        \n",
    "        for key,value in self.vocabulary.items():\n",
    "            if(value<threshold and len(tokenizer.tokenize(key))>1):\n",
    "                self.distinct_x-=1\n",
    "                emp_lis.append(key)\n",
    "                for labels in range(self.distinct_y):\n",
    "                    new_key=(key,labels)\n",
    "                    if(new_key in self.x_dict):\n",
    "                        count=self.x_dict[new_key]\n",
    "                        del self.x_dict[new_key]\n",
    "                        self.wordsInClass[labels]-=count\n",
    "        \n",
    "        for words in emp_lis:\n",
    "            del self.vocabulary[words]\n",
    "    \n",
    "    def getLogPrior(self,label):\n",
    "        return math.log(float(self.instancesInClass[label])/self.instances)\n",
    "    \n",
    "#     def getLogProb(self,attribute,label):\n",
    "        \n",
    "#         occurences=0\n",
    "#         key=(attribute,label)\n",
    "        \n",
    "#         if key in self.x_dict:\n",
    "#             occurences=self.x_dict[key]\n",
    "        \n",
    "#         occurences+=self.laplace_smoother\n",
    "\n",
    "#         total_occurences_in_class=self.wordsInClass[label]\n",
    "#         total_occurences_in_class+=self.distinct_x*self.laplace_smoother\n",
    "                    \n",
    "#         return math.log(float(occurences)/total_occurences_in_class)\n",
    "\n",
    "    def getLogProb(self,attribute,label):\n",
    "        \n",
    "        occurences=0\n",
    "        key=(attribute,label)\n",
    "        \n",
    "        if key in self.weight_dict:\n",
    "            occurences=self.weight_dict[key]\n",
    "        \n",
    "        occurences+=self.laplace_smoother\n",
    "\n",
    "        total_occurences_in_class=self.classweight[label]\n",
    "        total_occurences_in_class+=self.distinct_x*self.laplace_smoother\n",
    "            \n",
    "        return math.log(float(occurences)/total_occurences_in_class)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def getClass(self,x):\n",
    "        max_log_prob=-1e9\n",
    "        label=-1\n",
    "        \n",
    "        for i in range(self.distinct_y):\n",
    "            log_prob_x_given_y=0\n",
    "            \n",
    "            for attributes in x:\n",
    "                log_prob_x_given_y+=self.getLogProb(attributes,i)\n",
    "            \n",
    "            log_prob_x=log_prob_x_given_y+self.getLogPrior(i)\n",
    "            \n",
    "            if(log_prob_x>max_log_prob):\n",
    "                max_log_prob=log_prob_x\n",
    "                label=self.revMapy[i]\n",
    "            \n",
    "        return label\n",
    "    \n",
    "    def ConfusionMatrix(self,y,predy):\n",
    "        confusion = [ [0]*self.distinct_y for _ in range(self.distinct_y) ]\n",
    "        \n",
    "        tests=len(y)\n",
    "        \n",
    "        for i in range(tests):\n",
    "            confusion[self.y_dict[predy[i]]][self.y_dict[y[i]]]+=1\n",
    "        \n",
    "        for i,ii in self.y_dict.items():\n",
    "            for j,jj in self.y_dict.items():\n",
    "                print(\"%5d\"%(confusion[ii][jj]),end=' ')\n",
    "            print()\n",
    "        \n",
    "        \n",
    "    def getAccuracy(self,x,y,printConfusionMatrix=False):\n",
    "        total_tests=len(y)\n",
    "        passed_tests=0\n",
    "        prediction_list=[]\n",
    "        \n",
    "        for i in range(total_tests):\n",
    "            xi=x[i]\n",
    "            yi=y[i]\n",
    "            \n",
    "#             if y[i] in self.y_dict:\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 continue\n",
    "            pred_yi=self.getClass(xi)\n",
    "            prediction_list.append(pred_yi)\n",
    "            if(pred_yi==yi):\n",
    "                passed_tests+=1\n",
    "        \n",
    "        if(printConfusionMatrix==True):\n",
    "            self.ConfusionMatrix(y,pred_yi)\n",
    "                \n",
    "        return [prediction_list,(float(passed_tests))/total_tests]\n",
    "    \n",
    "    def getAccuracyRandomPredict(self,x,y):\n",
    "        \n",
    "        average_over=10\n",
    "        total_accuracy=0\n",
    "        \n",
    "        for i in range(average_over):\n",
    "            \n",
    "            total_tests=len(y)\n",
    "            passed_tests=0\n",
    "\n",
    "            for i in range(total_tests):\n",
    "                yi=y[i]\n",
    "\n",
    "                pred_yi=random.randint(0,self.distinct_y-1)\n",
    "                pred_yi=self.revMapy[pred_yi]\n",
    "\n",
    "                if(pred_yi==yi):\n",
    "                    passed_tests+=1\n",
    "\n",
    "            total_accuracy+=(float(passed_tests))/total_tests\n",
    "        \n",
    "        return total_accuracy/average_over\n",
    "    \n",
    "    def getAccuracyMajorityPredictor(self,x,y):\n",
    "        max_occ=-1\n",
    "        majority_class=-1\n",
    "        total_tests=len(y)\n",
    "        passed_tests=0        \n",
    "        \n",
    "        for i in range(self.distinct_y):\n",
    "            if(max_occ<self.instancesInClass[i]):\n",
    "                max_occ=self.instancesInClass[i]\n",
    "                majority_class=i\n",
    "        \n",
    "        if(majority_class==-1):\n",
    "            return 0\n",
    "        \n",
    "        majority_class=self.revMapy[majority_class]\n",
    "        \n",
    "        for yi in y:\n",
    "            if(yi==majority_class):\n",
    "                passed_tests+=1\n",
    "                \n",
    "        return (float(passed_tests))/total_tests       \n",
    "\n",
    "    def printParameters(self):\n",
    "        print(\"Vocabulary Size:\",self.distinct_x)\n",
    "        print(\"Classes:\",self.distinct_y)\n",
    "        print(\"X_Y's:\",len(self.x_dict))\n",
    "        \n",
    "        print(\"Mapped Classes:\", self.y_dict)\n",
    "        \n",
    "        print(\"Instances In Mapped class:\",end='')\n",
    "        \n",
    "        for counts in self.instancesInClass:\n",
    "            print(counts,end=' ')\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"Words In Mapped class:\",end='')\n",
    "        \n",
    "        for counts in self.wordsInClass:\n",
    "            print(counts,end=' ')\n",
    "        \n",
    "        print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(x_train,y_train):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    \n",
    "    instance=len(y_train)\n",
    "    lis1=[2,3,4]\n",
    "    lis2=[7,8,9]\n",
    "    \n",
    "    for i in range(instance):\n",
    "        x.append(x_train[i])\n",
    "        y.append(y_train[i])\n",
    "        \n",
    "        if(y_train[i]==1):\n",
    "            x.append(x_train[i])\n",
    "            y.append(lis1[random.randint(0,2)])\n",
    "            x.append(x_train[i])\n",
    "            y.append(lis1[random.randint(0,2)])\n",
    "#             x.append(x_train[i])\n",
    "#             y.append(lis1[random.randint(0,2)])\n",
    "        \n",
    "        if(y_train[i]==10):\n",
    "            x.append(x_train[i])\n",
    "            y.append(lis2[random.randint(0,2)])\n",
    "            x.append(x_train[i])\n",
    "            y.append(lis2[random.randint(0,2)])\n",
    "#             x.append(x_train[i])\n",
    "#             y.append(lis2[random.randint(0,2)])\n",
    "            \n",
    "    return (x,y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train=tokenizedInput_WithNGrams('Dataset/Stem/imdb_train_text.txt',grams=2)\n",
    "x_train=tokenizedInput('Dataset/Stem/imdb_train_text.txt')\n",
    "y_train=tokenizedInput('Dataset/imdb/imdb_train_labels.txt',tokenize=False)\n",
    "y_train=list(map(int,y_train))\n",
    "(x_train,y_train)=augmentation(x_train,y_train)\n",
    "# x_test=tokenizedInput_WithNGrams('Dataset/Stem/imdb_test_text.txt',grams=2)\n",
    "x_test=tokenizedInput('Dataset/Stem/imdb_test_text.txt')\n",
    "y_test=tokenizedInput('Dataset/imdb/imdb_test_labels.txt',tokenize=False)\n",
    "y_test=list(map(int,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44664\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199596\n",
      "50815\n",
      "50815\n"
     ]
    }
   ],
   "source": [
    "NaiveBayesClassifier=NaiveBayes(x=x_train,y=y_train)\n",
    "NaiveBayesClassifier.calculateParameters()\n",
    "print (len(NaiveBayesClassifier.x_dict))\n",
    "print(NaiveBayesClassifier.distinct_x)\n",
    "print(len(NaiveBayesClassifier.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199596\n",
      "50815\n",
      "50815\n"
     ]
    }
   ],
   "source": [
    "NaiveBayesClassifier.purge(threshold=7)\n",
    "NaiveBayesClassifier.tfidf()\n",
    "print(len(NaiveBayesClassifier.x_dict))\n",
    "print(NaiveBayesClassifier.distinct_x)\n",
    "print(len(NaiveBayesClassifier.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27312\n"
     ]
    }
   ],
   "source": [
    "[a,b]=(NaiveBayesClassifier.getAccuracy(x_test,y_test,printConfusionMatrix=False))\n",
    "print (b)\n",
    "# print(NaiveBayesClassifier.ConfusionMatrix(a,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1732   312   161    82    25    54    46   180 \n",
      " 1121   500   414   284   122   123   101   218 \n",
      " 1229   717   820   659   276   243   172   342 \n",
      "  730   602   808   974   382   314   196   296 \n",
      "   76    75   160   296   670   655   440   812 \n",
      "   82    63   130   267   659  1085   875  1545 \n",
      "   34    25    42    61   152   289   396   955 \n",
      "   18     8     6    12    21    87   118   651 \n"
     ]
    }
   ],
   "source": [
    "NaiveBayesClassifier.ConfusionMatrix(y_test,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaiveBayesClassifier.ConfusionMatrix(y_test,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 7, 2: 6, 3: 4, 4: 5, 7: 2, 8: 1, 9: 3, 10: 0}"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NaiveBayesClassifier.y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2=tokenizedInput('Dataset/Stem/imdb_train_text.txt')\n",
    "y_train2=tokenizedInput('Dataset/imdb/imdb_train_labels.txt',tokenize=False)\n",
    "y_train2=list(map(int,y_train2))\n",
    "(x_train2,y_train2)=augmentation(x_train2,y_train2)\n",
    "x_test2=tokenizedInput('Dataset/imdb/imdb_test_text.txt')\n",
    "y_test2=tokenizedInput('Dataset/imdb/imdb_test_labels.txt',tokenize=False)\n",
    "y_test2=list(map(int,y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaiveBayesClassifier2=NaiveBayes(x=x_train2,y=y_train2)\n",
    "NaiveBayesClassifier2.calculateParameters()\n",
    "# print (sum(NaiveBayesClassifier2.wordsInClass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24196\n"
     ]
    }
   ],
   "source": [
    "[a2,b2]=(NaiveBayesClassifier2.getAccuracy(x_test2,y_test2,printConfusionMatrix=False))\n",
    "print (b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1035   306   210   150    41    38    24    71 \n",
      "  375   185   162   149    55    60    51    99 \n",
      "    1     1     0     1     0     0     0     0 \n",
      "    0     0     0     0     0     0     0     0 \n",
      "    0     0     0     0     0     0     0     0 \n",
      "    0     0     0     0     0     0     0     0 \n",
      "    0     0     0     0     0     0     0     0 \n",
      " 3611  1810  2169  2335  2211  2752  2269  4829 \n"
     ]
    }
   ],
   "source": [
    "NaiveBayesClassifier.ConfusionMatrix(y_test2,a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train3=tokenizedInput_WithNGrams('Dataset/Stem/imdb_train_text.txt',grams=3)\n",
    "# y_train3=tokenizedInput('Dataset/imdb/imdb_train_labels.txt',tokenize=False)\n",
    "# y_train3=list(map(int,y_train3))\n",
    "# x_test3=tokenizedInput_WithNGrams('Dataset/imdb/imdb_test_text.txt',grams=3)\n",
    "# y_test3=tokenizedInput('Dataset/imdb/imdb_test_labels.txt',tokenize=False)\n",
    "# y_test3=list(map(int,y_test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2151085\n"
     ]
    }
   ],
   "source": [
    "# print (len(NaiveBayesClassifier2.x_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1=tokenizedInput_WithNGrams('Dataset/Clean1/imdb_train_text.txt',grams=1)\n",
    "y_train1=tokenizedInput('Dataset/imdb/imdb_train_labels.txt',tokenize=False)\n",
    "y_train1=list(map(int,y_train1))\n",
    "x_test1=tokenizedInput_WithNGrams('Dataset/Clean1/imdb_test_text.txt',grams=1)\n",
    "y_test1=tokenizedInput('Dataset/imdb/imdb_test_labels.txt',tokenize=False)\n",
    "y_test1=list(map(int,y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241067\n"
     ]
    }
   ],
   "source": [
    "NaiveBayesClassifier1=NaiveBayes(x=x_train1,y=y_train1)\n",
    "NaiveBayesClassifier1.calculateParameters()\n",
    "print (len(NaiveBayesClassifier1.x_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68476\n"
     ]
    }
   ],
   "source": [
    "[a1,b1]=(NaiveBayesClassifier1.getAccuracy(x_train1,y_train1,printConfusionMatrix=False))\n",
    "print (b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3065  1374  1548  1581   242   291   231   528 \n",
      "  383   172   178   162     6     6     1     3 \n",
      "  564   265   281   315     7    10     9    20 \n",
      "  737   381   416   430    25    24    36    52 \n",
      "   54    26    21    36   335   378   304   728 \n",
      "   69    34    35    45   432   527   436   913 \n",
      "   10     3     5     2   183   240   218   412 \n",
      "  140    47    57    64  1077  1374  1109  2343 \n"
     ]
    }
   ],
   "source": [
    "NaiveBayesClassifier1.ConfusionMatrix(y_test1,a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_uni_bi=tokenizedInput_WithNGrams('Dataset/Stem/imdb_train_text.txt',grams=2)\n",
    "y_train_uni_bi=tokenizedInput('Dataset/imdb/imdb_train_labels.txt',tokenize=False)\n",
    "y_train_uni_bi=list(map(int,y_train_uni_bi))\n",
    "x_test_uni_bi=tokenizedInput_WithNGrams('Dataset/Stem/imdb_test_text.txt',grams=2)\n",
    "y_test_uni_bi=tokenizedInput('Dataset/imdb/imdb_test_labels.txt',tokenize=False)\n",
    "y_test_uni_bi=list(map(int,y_test1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
