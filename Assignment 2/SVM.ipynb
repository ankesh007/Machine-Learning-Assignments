{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpd=pd.read_csv(\"Dataset/mnist/train.csv\",header=None,sep=',')\n",
    "testpd=pd.read_csv(\"Dataset/mnist/test.csv\",header=None,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    \n",
    "    def __init__(self,x,y,normalize_x=True):\n",
    "        self.x=np.copy(x).astype(float)\n",
    "        self.y=np.copy(y)\n",
    "        [instances,dimensions]=x.shape\n",
    "        self.dimensions=dimensions\n",
    "        self.instances=instances\n",
    "        self.normalize_param=np.ones((1,dimensions))\n",
    "        self.normalize_x=normalize_x\n",
    "        label=np.unique(y).shape[0]\n",
    "        self.label=label\n",
    "        self.weight=np.zeros((self.label,self.label,dimensions))\n",
    "        self.bias=np.zeros((self.label,self.label))\n",
    "        self.epoch=np.zeros((self.label,self.label))\n",
    "        self.train_steps=np.zeros((self.label,self.label))\n",
    "        self.isTrained=np.zeros((self.label,self.label))\n",
    "\n",
    "        if(normalize_x==True):\n",
    "            self.normalize_param*=255\n",
    "            self.x=self.normalize(self.x)\n",
    "        \n",
    "    def normalize(self,x):\n",
    "        return (x/self.normalize_param)\n",
    "    \n",
    "    def resetParam(self,i,j):\n",
    "        self.weight[i,j,:]*=0\n",
    "        self.bias[i,j]=0\n",
    "        self.epoch[i,j]=0\n",
    "        self.train_steps[i,j]=0\n",
    "        self.isTrained[i,j]=0\n",
    "        return\n",
    "    \n",
    "    def evaluate(self,x,y,weight,bias):\n",
    "        return (np.matmul(x,weight)+bias)*y\n",
    "    \n",
    "    def getLoss(self,x,y,lamda,C,weight,bias):\n",
    "        loss=np.matmul(np.transpose(weight),weight)*lamda\n",
    "        evaluate=1-self.evaluate(x=x,y=y,weight=weight,bias=bias)\n",
    "        mask=(evaluate>0).astype(int)\n",
    "        evaluate*=mask\n",
    "        loss+=C*np.sum(evaluate)\n",
    "        return loss\n",
    "    \n",
    "    def getWeight(self,i,j):\n",
    "        return self.weight[i,j,:][:,np.newaxis]\n",
    "    \n",
    "    def train1Classier(self,weight,bias,train_steps,epoch,x=None,y=None,lamda=1,C=1,max_steps=2000,batch_mode=True,batch_size=100,epsilon=0.001,log_every=1000):\n",
    "\n",
    "        if(x is None):\n",
    "            x=self.x\n",
    "        if(y is None):\n",
    "            y=self.y\n",
    "            \n",
    "        xy=-(x*y)\n",
    "        [instances,dim]=x.shape\n",
    "        [instances,ydim]=y.shape\n",
    "        \n",
    "        if(batch_mode==False):\n",
    "            batch_size=instances\n",
    "        \n",
    "        prev_loss=self.getLoss(x=x,y=y,lamda=lamda,C=C,weight=weight,bias=bias)\n",
    "            \n",
    "        Trained=False    \n",
    "        cur_steps=0\n",
    "        while(Trained==False and cur_steps<max_steps):\n",
    "            cur_instance=0\n",
    "            epoch+=1\n",
    "            \n",
    "            while(cur_instance<instances and cur_steps<max_steps):\n",
    "                train_steps+=1\n",
    "                cur_steps+=1\n",
    "                eta=1.0/(cur_steps)\n",
    "                up_lim=min(instances,cur_instance+batch_size)\n",
    "                batch_x=x[cur_instance:up_lim,0:dim]\n",
    "                batch_y=y[cur_instance:up_lim,0:ydim]\n",
    "                batch_xy=xy[cur_instance:up_lim,0:dim]\n",
    "                examples=up_lim-cur_instance\n",
    "                cur_instance=up_lim\n",
    "                evaluate=self.evaluate(batch_x,batch_y,weight,bias)\n",
    "                mask=((1-evaluate)>=0).astype(int)\n",
    "                weight=(1-eta*lamda)*weight - (eta*C*(np.sum(batch_xy*mask,axis=0)[:,np.newaxis]))\n",
    "                bias=bias+((np.sum(mask*batch_y))*C*eta)\n",
    "                \n",
    "                if(train_steps%log_every==0):\n",
    "                    print(train_steps)\n",
    "#                     print(\"Batch Loss at Steps=\",train_steps,\" is \",self.getLoss(x,y,lamda,C,weight,bias))\n",
    "            \n",
    "            cur_loss=self.getLoss(x,y,lamda,C,weight,bias)\n",
    "            if(abs(cur_loss-prev_loss)<epsilon):\n",
    "                prev_loss=cur_loss\n",
    "                Trained=True\n",
    "                \n",
    "            prev_loss=cur_loss\n",
    "        \n",
    "        print(\"Loss on Ending:\",prev_loss)\n",
    "\n",
    "        return (weight,bias,epoch,train_steps,Trained)\n",
    "    \n",
    "    def trainIJclassifier(self,i,j,reset=False):\n",
    "        \n",
    "        if(reset==True):\n",
    "            self.resetParam(i,j)\n",
    "        \n",
    "        print(\"********************************************************\")\n",
    "        print(i,\" \",j,\" \",\"Classifier\")\n",
    "        \n",
    "        if(self.isTrained[i,j]==1):\n",
    "            return\n",
    "        \n",
    "        aux_arr=(self.y==i).astype(int)+(self.y==j).astype(int)\n",
    "\n",
    "        newx=self.x[aux_arr[:,0]>0]\n",
    "        newy=self.y[aux_arr[:,0]>0]\n",
    "        newy2=np.copy(newy)\n",
    "        newy=((newy2==i).astype(int))-((newy2==j).astype(int))\n",
    "\n",
    "        (weight,bias,epoch,train_steps,Trained)=self.train1Classier(weight=self.getWeight(i,j),\n",
    "                                          bias=self.bias[i,j],\n",
    "                                          x=newx,\n",
    "                                          y=newy,\n",
    "                                          train_steps=self.train_steps[i,j],\n",
    "                                          epoch=self.epoch[i,j])\n",
    "        \n",
    "        self.weight[i,j,:]=weight[:,0]\n",
    "        self.bias[i,j]=bias\n",
    "        self.epoch[i,j]=epoch\n",
    "        self.train_steps[i,j]=train_steps\n",
    "        \n",
    "        if(Trained==True):\n",
    "            self.isTrained[i,j]=1\n",
    "        \n",
    "    def train(self):\n",
    "        for i in range(self.label):\n",
    "            for j in range(i+1,self.label,1):\n",
    "                self.trainIJclassifier(i,j)\n",
    "\n",
    "    def getClass(self,x,i,j):\n",
    "        weight=self.getWeight(i,j)\n",
    "        \n",
    "        if(np.matmul(weight.T,x)+self.bias[i,j]>0):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "                \n",
    "    def predictInstance(self,x):\n",
    "        \n",
    "        count=[]\n",
    "        for i in range(self.label):\n",
    "            count.append(0)\n",
    "            \n",
    "        for i in range(self.label):\n",
    "            for j in range(i+1,self.label,1):\n",
    "                aux_class=self.getClass(x,i,j)\n",
    "                if(aux_class==1):\n",
    "                    count[i]+=1\n",
    "                else:\n",
    "                    count[j]+=1\n",
    "        \n",
    "        coun=-1\n",
    "        lab=-1\n",
    "        \n",
    "        for i in range(self.label):\n",
    "\n",
    "            if(coun<=count[i]):\n",
    "                coun=count[i]\n",
    "                lab=i\n",
    "        return lab\n",
    "    \n",
    "    def predict(self,x):\n",
    "        predictions=[]\n",
    "        \n",
    "        instances=x.shape[0]\n",
    "        \n",
    "        for i in range(instances):\n",
    "            predictions.append(self.predictInstance(x[i][::,np.newaxis]))\n",
    "        \n",
    "        return np.asarray(predictions)[:,np.newaxis]\n",
    "            \n",
    "    def getAccuracy(self,x,y):\n",
    "        predictions=self.predict(x.astype(float)/self.normalize_param)\n",
    "        \n",
    "        mask_correct=np.sum((predictions==y).astype(int))\n",
    "        instances=y.shape[0]\n",
    "        \n",
    "        return float(mask_correct)/instances\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDataframe(dataframe):\n",
    "    values=dataframe.values\n",
    "    [instances,dim]=values.shape\n",
    "    x=values[:,0:dim-1]\n",
    "    y=values[:,dim-1:dim]\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train)=parseDataframe(trainpd)\n",
    "(x_test,y_test)=parseDataframe(testpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 784)\n",
      "(20000, 1)\n",
      "(10000, 784)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_instance=SVM(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************\n",
      "0   1   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[99.35599429]]\n",
      "********************************************************\n",
      "0   2   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[120.19467204]]\n",
      "********************************************************\n",
      "0   3   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[92.6640639]]\n",
      "********************************************************\n",
      "0   4   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[208.49730042]]\n",
      "********************************************************\n",
      "0   5   Classifier\n",
      "5000.0\n",
      "6000.0\n",
      "[[206.93188648]]\n",
      "********************************************************\n",
      "0   6   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[93.34738301]]\n",
      "********************************************************\n",
      "0   7   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[106.96184821]]\n",
      "********************************************************\n",
      "0   8   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[68.75287147]]\n",
      "********************************************************\n",
      "0   9   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[72.58533908]]\n",
      "********************************************************\n",
      "1   2   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[158.2780621]]\n",
      "********************************************************\n",
      "1   3   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[85.87704667]]\n",
      "********************************************************\n",
      "1   4   Classifier\n",
      "[[114.04544]]\n",
      "********************************************************\n",
      "1   5   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[88.43976001]]\n",
      "********************************************************\n",
      "1   6   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[39.36938298]]\n",
      "********************************************************\n",
      "1   7   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[140.03212783]]\n",
      "********************************************************\n",
      "1   8   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[156.72715895]]\n",
      "********************************************************\n",
      "1   9   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[42.6235714]]\n",
      "********************************************************\n",
      "2   3   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[358.32774833]]\n",
      "********************************************************\n",
      "2   4   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[149.13081667]]\n",
      "********************************************************\n",
      "2   5   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[233.15013194]]\n",
      "********************************************************\n",
      "2   6   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[194.71970602]]\n",
      "********************************************************\n",
      "2   7   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[121.99645998]]\n",
      "********************************************************\n",
      "2   8   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[255.65459339]]\n",
      "********************************************************\n",
      "2   9   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[112.21805005]]\n",
      "********************************************************\n",
      "3   4   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[48.25693385]]\n",
      "********************************************************\n",
      "3   5   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[409.56068018]]\n",
      "********************************************************\n",
      "3   6   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[60.86253036]]\n",
      "********************************************************\n",
      "3   7   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[216.04965747]]\n",
      "********************************************************\n",
      "3   8   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[468.30344354]]\n",
      "********************************************************\n",
      "3   9   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[154.00415024]]\n",
      "********************************************************\n",
      "4   5   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[106.78344181]]\n",
      "********************************************************\n",
      "4   6   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[206.26845424]]\n",
      "********************************************************\n",
      "4   7   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[136.21373917]]\n",
      "********************************************************\n",
      "4   8   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[61.69041673]]\n",
      "********************************************************\n",
      "4   9   Classifier\n",
      "5000.0\n",
      "6000.0\n",
      "[[349.52349495]]\n",
      "********************************************************\n",
      "5   6   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[186.90773608]]\n",
      "********************************************************\n",
      "5   7   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[100.33137399]]\n",
      "********************************************************\n",
      "5   8   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[346.95363006]]\n",
      "********************************************************\n",
      "5   9   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[193.81495782]]\n",
      "********************************************************\n",
      "6   7   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[208.53529543]]\n",
      "********************************************************\n",
      "6   8   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[109.11595129]]\n",
      "********************************************************\n",
      "6   9   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[73.31494207]]\n",
      "********************************************************\n",
      "7   8   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[322.90767483]]\n",
      "********************************************************\n",
      "7   9   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[435.25617389]]\n",
      "********************************************************\n",
      "8   9   Classifier\n",
      "7000.0\n",
      "8000.0\n",
      "[[158.10444769]]\n"
     ]
    }
   ],
   "source": [
    "SVM_instance.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9193"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_instance.getAccuracy(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=SVM_instance.weight\n",
    "bias=SVM_instance.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_instance.weight=weights\n",
    "SVM_bias=bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
