{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,inputNodes,hiddenList,hiddenActivations,outputNodes=1,outputActivation=\"SIGMOID\"):\n",
    "        self.inputNodes=inputNodes\n",
    "        self.outputNodes=outputNodes\n",
    "        self.hiddenList=[inputNodes]+hiddenList+[outputNodes]\n",
    "        # Nodes in each Layer (Also stores corresponding to input and output layer)\n",
    "        self.Activations=[\"None\"]+hiddenActivations+[outputActivation]\n",
    "        # Activation Functions (Also stores corresponding to input and output layer)\n",
    "        self.weight=[]\n",
    "        self.os=[]\n",
    "        self.net=[]\n",
    "        self.deltas=[]\n",
    "        self.loss=0\n",
    "        self.trainsteps=0\n",
    "        self.epoch=0\n",
    "    \n",
    "    # Initialise weight Matrices Randomly\n",
    "    def initialiseWeight(self):\n",
    "        layers=len(self.hiddenList)\n",
    "        self.trainsteps=0\n",
    "        self.epoch=0\n",
    "        \n",
    "        for i in range(1,layers,1):\n",
    "            self.weight.append(np.random.randn(self.hiddenList[i],self.hiddenList[i-1]+1))\n",
    "    \n",
    "    # Applies Activation Function\n",
    "    def applyActivation(self,vector,activation=\"None\"):\n",
    "        if(activation==\"RELU\"):\n",
    "            return vector*(vector>=0)\n",
    "        elif(activation==\"SIGMOID\"):\n",
    "            return (1.0/(1+np.exp(-vector)))\n",
    "        else:\n",
    "            return 1*vector\n",
    "    \n",
    "    def append1(self,x):\n",
    "        [instances,attr]=x.shape\n",
    "        bias=np.ones([instances,1])\n",
    "        return np.concatenate((bias,x),axis=1)\n",
    "    \n",
    "    # Forward Pass a batch through the network\n",
    "    def forwardPass(self,batchX):\n",
    "        \n",
    "        layers=len(self.weight)\n",
    "        self.os=[self.append1(batchX)]\n",
    "        self.net=[self.append1(batchX)]\n",
    "                \n",
    "        for i in range(layers):\n",
    "            x=self.os[i]\n",
    "            netj=np.matmul(x,self.weight[i].T)\n",
    "            o=(self.applyActivation(netj,self.Activations[i+1]))\n",
    "            if(i==layers-1):\n",
    "                self.net.append(netj)\n",
    "                self.os.append(o)\n",
    "            else:\n",
    "                self.net.append(self.append1(netj))\n",
    "                self.os.append(self.append1(o))\n",
    "    \n",
    "    # Euclidean Loss -> if Changed, gradient J wrt o must be updated\n",
    "    def lossfunction(self,pred,y):\n",
    "        return (pred-y)**2\n",
    "    \n",
    "    # Function returning loss on batch\n",
    "    def getLoss(self,batchX,batchY):\n",
    "        self.forwardPass(batchX)\n",
    "        layers=len(self.os)\n",
    "        finalOutput=self.os[layers-1]\n",
    "        return np.sum(self.lossfunction(finalOutput,batchY))\n",
    "\n",
    "    # Gradient of o wrt net \n",
    "    def gradient_o_net(self,o,net,activation=\"None\"):\n",
    "        if(activation==\"RELU\"):\n",
    "            return (net>=0)\n",
    "        elif(activation==\"SIGMOID\"):\n",
    "            return o*(1-o)\n",
    "        else:\n",
    "            return (o==o)\n",
    "    \n",
    "    def gradient_o_net_layer(self,layerNo):\n",
    "        return self.gradient_o_net(self.os[layerNo],self.net[layerNo],self.Activations[layerNo])\n",
    "    \n",
    "    # Gradient of J wrt o\n",
    "    def gradient_J_o(self,o,y):\n",
    "        return 2*(o-y)        \n",
    "\n",
    "    # Backward Pass a batch through the network updating Weight Matrices   \n",
    "    def backwardPass(self,batchX,batchY,learningRate=0.01):\n",
    "                \n",
    "        layers=len(self.os)\n",
    "#         m=float(1)\n",
    "        m=float((batchX.shape)[0])\n",
    "        self.deltas=[]\n",
    "        \n",
    "        finalDelta=self.gradient_J_o(self.os[layers-1],batchY)*self.gradient_o_net_layer(layers-1)        \n",
    "        self.deltas.append(finalDelta)\n",
    "        weightMatrixLen=len(self.weight)\n",
    "        \n",
    "        for i in range(layers-2,0,-1):\n",
    "            weightMatrixLen-=1\n",
    "            current_o_net=self.gradient_o_net_layer(i)\n",
    "            current_delta=np.matmul(self.deltas[0],self.weight[weightMatrixLen])\n",
    "            current_delta=current_delta*current_o_net\n",
    "            [_,attributes]=current_delta.shape\n",
    "            self.deltas=[current_delta[:,1:attributes]]+self.deltas\n",
    "        \n",
    "        layers=len(self.weight)\n",
    "         \n",
    "        for i in range(layers-1,-1,-1):\n",
    "            del_w=np.matmul(self.deltas[i].T,self.os[i])\n",
    "            self.weight[i]-=(learningRate/m)*del_w\n",
    "\n",
    "    # training the NN\n",
    "    def train(self,X,Y,learningRate=0.01,batchMode=False,batchSize=100,epsilon=0.0001,epochsToRun=500):\n",
    "        \n",
    "        [instances,attributes]=X.shape\n",
    "        [_,outputs]=Y.shape\n",
    "        \n",
    "        if(batchMode==False):\n",
    "            batchSize=instances\n",
    "        \n",
    "        Trained=False\n",
    "        prevLoss=self.getLoss(X,Y)\n",
    "        epochs=0\n",
    "        \n",
    "        while(not Trained and epochs<epochsToRun):\n",
    "            self.epoch+=1\n",
    "            epochs+=1\n",
    "            \n",
    "            cur=0\n",
    "            while(cur<instances):\n",
    "                uplim=min(cur+batchSize,instances)\n",
    "                batchX=X[cur:uplim,0:attributes]\n",
    "                batchY=Y[cur:uplim,0:outputs]\n",
    "                cur=uplim\n",
    "                self.forwardPass(batchX)\n",
    "                self.backwardPass(batchX,batchY,learningRate)\n",
    "                self.trainsteps+=1\n",
    "            \n",
    "            curLoss=self.getLoss(X,Y)\n",
    "            \n",
    "            if(abs(curLoss-prevLoss)<epsilon):\n",
    "                Trained=True\n",
    "            \n",
    "            prevLoss=curLoss\n",
    "            if(self.epoch%20==0):\n",
    "                print (\"Epoch:\",self.epoch,\"Loss:\",prevLoss,\"Accuracy:\",self.getAccuracy(X,Y))\n",
    "        \n",
    "    def predict(self,X):\n",
    "        self.forwardPass(X)\n",
    "        layers=len(self.os)\n",
    "        return self.os[layers-1]\n",
    "    \n",
    "    def getAccuracy(self,X,Y):\n",
    "        pred=self.predict(X)\n",
    "        instances=float(X.shape[0])\n",
    "        \n",
    "        pred=(pred>=0.5).astype(int)\n",
    "        \n",
    "        return (np.sum(pred==Y))/instances\n",
    "        \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain=pd.read_csv(\"Dataset/NN/toy_data/toy_trainX.csv\",header=None,sep=',').values\n",
    "ytrain=pd.read_csv(\"Dataset/NN/toy_data/toy_trainY.csv\",header=None,sep=',').values\n",
    "xtest=pd.read_csv(\"Dataset/NN/toy_data/toy_testX.csv\",header=None,sep=',').values\n",
    "ytest=pd.read_csv(\"Dataset/NN/toy_data/toy_testY.csv\",header=None,sep=',').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xtrain=pd.read_csv(\"Dataset/NN/mnist_data/MNIST_train.csv\",header=None,sep=',').values\n",
    "# ytrain=xtrain[:,784:785]\n",
    "# xtrain=xtrain[:,0:784]\n",
    "# xtest=pd.read_csv(\"Dataset/NN/mnist_data/MNIST_test.csv\",header=None,sep=',').values\n",
    "# ytest=xtest[:,784:785]\n",
    "# xtest=xtest[:,0:784]\n",
    "# ytrain=(ytrain==6).astype(int)\n",
    "# ytest=(ytest==6).astype(int)\n",
    "# xtrain=xtrain/255.0\n",
    "# xtest=xtest/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(380, 2)\n",
      "(380, 1)\n",
      "(120, 2)\n",
      "(120, 1)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "print(xtest.shape)\n",
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN_obj=NeuralNetwork(xtrain.shape[1],[5,5],[\"SIGMOID\",\"SIGMOID\"],ytrain.shape[1])\n",
    "# NN_obj=NeuralNetwork(xtrain.shape[1],[5,5],[\"RELU\",\"RELU\"],ytrain.shape[1])\n",
    "# NN_obj=NeuralNetwork(xtrain.shape[1],[5],[\"SIGMOID\"],ytrain.shape[1])\n",
    "NN_obj=NeuralNetwork(xtrain.shape[1],[5],[\"SIGMOID\"],ytrain.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_obj.initialiseWeight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3)\n",
      "(1, 6)\n"
     ]
    }
   ],
   "source": [
    "for weights in NN_obj.weight:\n",
    "    print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114.14875075963997"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_obj.getLoss(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51160 Loss: 39.42597430086405 Accuracy: 0.8789473684210526\n",
      "Epoch: 51180 Loss: 39.238898155756054 Accuracy: 0.8789473684210526\n",
      "Epoch: 51200 Loss: 39.059368729494324 Accuracy: 0.8789473684210526\n",
      "Epoch: 51220 Loss: 38.88764585554378 Accuracy: 0.8789473684210526\n",
      "Epoch: 51240 Loss: 38.723326047737785 Accuracy: 0.881578947368421\n",
      "Epoch: 51260 Loss: 38.56600392234988 Accuracy: 0.881578947368421\n",
      "Epoch: 51280 Loss: 38.41530699895039 Accuracy: 0.881578947368421\n",
      "Epoch: 51300 Loss: 38.27089253547222 Accuracy: 0.881578947368421\n",
      "Epoch: 51320 Loss: 38.13244236069398 Accuracy: 0.881578947368421\n",
      "Epoch: 51340 Loss: 37.99965870062045 Accuracy: 0.8842105263157894\n",
      "Epoch: 51360 Loss: 37.87226127606496 Accuracy: 0.8842105263157894\n",
      "Epoch: 51380 Loss: 37.74998552298939 Accuracy: 0.8842105263157894\n",
      "Epoch: 51400 Loss: 37.63258167453108 Accuracy: 0.8842105263157894\n",
      "Epoch: 51420 Loss: 37.51981440455655 Accuracy: 0.8842105263157894\n",
      "Epoch: 51440 Loss: 37.41146272065017 Accuracy: 0.8842105263157894\n",
      "Epoch: 51460 Loss: 37.30731980503492 Accuracy: 0.8842105263157894\n",
      "Epoch: 51480 Loss: 37.20719254415965 Accuracy: 0.8842105263157894\n",
      "Epoch: 51500 Loss: 37.11090057210431 Accuracy: 0.8868421052631579\n",
      "Epoch: 51520 Loss: 37.018274780029024 Accuracy: 0.8868421052631579\n",
      "Epoch: 51540 Loss: 36.92915539480993 Accuracy: 0.8868421052631579\n",
      "Epoch: 51560 Loss: 36.843389866049094 Accuracy: 0.8868421052631579\n",
      "Epoch: 51580 Loss: 36.76083087593884 Accuracy: 0.8868421052631579\n",
      "Epoch: 51600 Loss: 36.68133477034435 Accuracy: 0.8868421052631579\n",
      "Epoch: 51620 Loss: 36.60476060552247 Accuracy: 0.8868421052631579\n",
      "Epoch: 51640 Loss: 36.53096985247583 Accuracy: 0.8868421052631579\n",
      "Epoch: 51660 Loss: 36.45982665624466 Accuracy: 0.8868421052631579\n",
      "Epoch: 51680 Loss: 36.39119845603547 Accuracy: 0.8842105263157894\n",
      "Epoch: 51700 Loss: 36.32495675050668 Accuracy: 0.8842105263157894\n",
      "Epoch: 51720 Loss: 36.26097782809471 Accuracy: 0.8842105263157894\n",
      "Epoch: 51740 Loss: 36.19914334679787 Accuracy: 0.8842105263157894\n",
      "Epoch: 51760 Loss: 36.13934071383552 Accuracy: 0.8842105263157894\n",
      "Epoch: 51780 Loss: 36.08146326595056 Accuracy: 0.8842105263157894\n",
      "Epoch: 51800 Loss: 36.02541028053059 Accuracy: 0.8868421052631579\n",
      "Epoch: 51820 Loss: 35.97108685900194 Accuracy: 0.8868421052631579\n",
      "Epoch: 51840 Loss: 35.91840372322088 Accuracy: 0.8868421052631579\n",
      "Epoch: 51860 Loss: 35.86727695882868 Accuracy: 0.8868421052631579\n",
      "Epoch: 51880 Loss: 35.81762773103189 Accuracy: 0.8868421052631579\n",
      "Epoch: 51900 Loss: 35.76938199041314 Accuracy: 0.8868421052631579\n",
      "Epoch: 51920 Loss: 35.722470180142295 Accuracy: 0.8868421052631579\n",
      "Epoch: 51940 Loss: 35.67682695146445 Accuracy: 0.8868421052631579\n",
      "Epoch: 51960 Loss: 35.63239089132166 Accuracy: 0.8868421052631579\n",
      "Epoch: 51980 Loss: 35.58910426404516 Accuracy: 0.8868421052631579\n",
      "Epoch: 52000 Loss: 35.54691276788489 Accuracy: 0.8868421052631579\n",
      "Epoch: 52020 Loss: 35.50576530645754 Accuracy: 0.8868421052631579\n",
      "Epoch: 52040 Loss: 35.4656137748046 Accuracy: 0.8894736842105263\n",
      "Epoch: 52060 Loss: 35.4264128595385 Accuracy: 0.8894736842105263\n",
      "Epoch: 52080 Loss: 35.388119852442316 Accuracy: 0.8894736842105263\n",
      "Epoch: 52100 Loss: 35.35069447683361 Accuracy: 0.8894736842105263\n",
      "Epoch: 52120 Loss: 35.314098725982184 Accuracy: 0.8894736842105263\n",
      "Epoch: 52140 Loss: 35.278296712868915 Accuracy: 0.8894736842105263\n",
      "Epoch: 52160 Loss: 35.24325453058377 Accuracy: 0.8894736842105263\n",
      "Epoch: 52180 Loss: 35.20894012267908 Accuracy: 0.8894736842105263\n",
      "Epoch: 52200 Loss: 35.175323162818046 Accuracy: 0.8894736842105263\n",
      "Epoch: 52220 Loss: 35.142374943086 Accuracy: 0.8894736842105263\n",
      "Epoch: 52240 Loss: 35.11006827036188 Accuracy: 0.8921052631578947\n",
      "Epoch: 52260 Loss: 35.078377370179865 Accuracy: 0.8921052631578947\n",
      "Epoch: 52280 Loss: 35.04727779754289 Accuracy: 0.8921052631578947\n",
      "Epoch: 52300 Loss: 35.01674635418361 Accuracy: 0.8921052631578947\n",
      "Epoch: 52320 Loss: 34.986761011800326 Accuracy: 0.8921052631578947\n",
      "Epoch: 52340 Loss: 34.95730084082746 Accuracy: 0.8921052631578947\n",
      "Epoch: 52360 Loss: 34.92834594433112 Accuracy: 0.8921052631578947\n",
      "Epoch: 52380 Loss: 34.89987739664922 Accuracy: 0.8921052631578947\n",
      "Epoch: 52400 Loss: 34.87187718642436 Accuracy: 0.8921052631578947\n",
      "Epoch: 52420 Loss: 34.844328163703125 Accuracy: 0.8921052631578947\n",
      "Epoch: 52440 Loss: 34.81721399080091 Accuracy: 0.8921052631578947\n",
      "Epoch: 52460 Loss: 34.790519096654066 Accuracy: 0.8921052631578947\n",
      "Epoch: 52480 Loss: 34.76422863440267 Accuracy: 0.8921052631578947\n",
      "Epoch: 52500 Loss: 34.738328441966935 Accuracy: 0.8921052631578947\n",
      "Epoch: 52520 Loss: 34.71280500539877 Accuracy: 0.8921052631578947\n",
      "Epoch: 52540 Loss: 34.68764542480642 Accuracy: 0.8921052631578947\n",
      "Epoch: 52560 Loss: 34.66283738266594 Accuracy: 0.8921052631578947\n",
      "Epoch: 52580 Loss: 34.63836911434693 Accuracy: 0.8921052631578947\n",
      "Epoch: 52600 Loss: 34.61422938069295 Accuracy: 0.8921052631578947\n",
      "Epoch: 52620 Loss: 34.590407442508756 Accuracy: 0.8921052631578947\n",
      "Epoch: 52640 Loss: 34.56689303681695 Accuracy: 0.8921052631578947\n",
      "Epoch: 52660 Loss: 34.54367635475613 Accuracy: 0.8921052631578947\n",
      "Epoch: 52680 Loss: 34.52074802100153 Accuracy: 0.8921052631578947\n",
      "Epoch: 52700 Loss: 34.49809907459647 Accuracy: 0.8921052631578947\n",
      "Epoch: 52720 Loss: 34.47572095109033 Accuracy: 0.8921052631578947\n",
      "Epoch: 52740 Loss: 34.453605465884735 Accuracy: 0.8921052631578947\n",
      "Epoch: 52760 Loss: 34.43174479869496 Accuracy: 0.8921052631578947\n",
      "Epoch: 52780 Loss: 34.41013147903925 Accuracy: 0.8921052631578947\n",
      "Epoch: 52800 Loss: 34.388758372672065 Accuracy: 0.8921052631578947\n",
      "Epoch: 52820 Loss: 34.367618668882336 Accuracy: 0.8921052631578947\n",
      "Epoch: 52840 Loss: 34.346705868580166 Accuracy: 0.8921052631578947\n",
      "Epoch: 52860 Loss: 34.326013773099405 Accuracy: 0.8921052631578947\n",
      "Epoch: 52880 Loss: 34.305536473645866 Accuracy: 0.8921052631578947\n",
      "Epoch: 52900 Loss: 34.28526834132357 Accuracy: 0.8947368421052632\n",
      "Epoch: 52920 Loss: 34.26520401767385 Accuracy: 0.8947368421052632\n",
      "Epoch: 52940 Loss: 34.24533840566444 Accuracy: 0.8947368421052632\n",
      "Epoch: 52960 Loss: 34.22566666106775 Accuracy: 0.8947368421052632\n",
      "Epoch: 52980 Loss: 34.206184184169814 Accuracy: 0.8947368421052632\n",
      "Epoch: 53000 Loss: 34.18688661175368 Accuracy: 0.8947368421052632\n",
      "Epoch: 53020 Loss: 34.16776980930353 Accuracy: 0.8947368421052632\n",
      "Epoch: 53040 Loss: 34.148829863378154 Accuracy: 0.8947368421052632\n",
      "Epoch: 53060 Loss: 34.1300630741056 Accuracy: 0.8947368421052632\n",
      "Epoch: 53080 Loss: 34.11146594775384 Accuracy: 0.8947368421052632\n",
      "Epoch: 53100 Loss: 34.09303518933561 Accuracy: 0.8947368421052632\n",
      "Epoch: 53120 Loss: 34.07476769520988 Accuracy: 0.8947368421052632\n",
      "Epoch: 53140 Loss: 34.056660545646224 Accuracy: 0.8947368421052632\n",
      "Epoch: 53160 Loss: 34.038710997323406 Accuracy: 0.8947368421052632\n",
      "Epoch: 53180 Loss: 34.02091647573821 Accuracy: 0.8947368421052632\n",
      "Epoch: 53200 Loss: 34.00327456750637 Accuracy: 0.8947368421052632\n",
      "Epoch: 53220 Loss: 33.98578301254285 Accuracy: 0.8947368421052632\n",
      "Epoch: 53240 Loss: 33.968439696115155 Accuracy: 0.8947368421052632\n",
      "Epoch: 53260 Loss: 33.95124264076918 Accuracy: 0.8947368421052632\n",
      "Epoch: 53280 Loss: 33.93418999813401 Accuracy: 0.8947368421052632\n",
      "Epoch: 53300 Loss: 33.91728004061795 Accuracy: 0.8947368421052632\n",
      "Epoch: 53320 Loss: 33.90051115301482 Accuracy: 0.8947368421052632\n",
      "Epoch: 53340 Loss: 33.88388182404496 Accuracy: 0.8947368421052632\n",
      "Epoch: 53360 Loss: 33.86739063786202 Accuracy: 0.8947368421052632\n",
      "Epoch: 53380 Loss: 33.85103626556064 Accuracy: 0.8947368421052632\n",
      "Epoch: 53400 Loss: 33.83481745672536 Accuracy: 0.8947368421052632\n",
      "Epoch: 53420 Loss: 33.81873303106471 Accuracy: 0.8947368421052632\n",
      "Epoch: 53440 Loss: 33.802781870176986 Accuracy: 0.8947368421052632\n",
      "Epoch: 53460 Loss: 33.786962909496374 Accuracy: 0.8947368421052632\n",
      "Epoch: 53480 Loss: 33.77127513046856 Accuracy: 0.8947368421052632\n",
      "Epoch: 53500 Loss: 33.755717553005 Accuracy: 0.8947368421052632\n",
      "Epoch: 53520 Loss: 33.740289228263336 Accuracy: 0.8947368421052632\n",
      "Epoch: 53540 Loss: 33.724989231799036 Accuracy: 0.8947368421052632\n",
      "Epoch: 53560 Loss: 33.70981665712968 Accuracy: 0.8947368421052632\n",
      "Epoch: 53580 Loss: 33.69477060974909 Accuracy: 0.8947368421052632\n",
      "Epoch: 53600 Loss: 33.67985020162281 Accuracy: 0.8947368421052632\n",
      "Epoch: 53620 Loss: 33.665054546190525 Accuracy: 0.8947368421052632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53640 Loss: 33.65038275389479 Accuracy: 0.8947368421052632\n",
      "Epoch: 53660 Loss: 33.63583392824772 Accuracy: 0.8947368421052632\n",
      "Epoch: 53680 Loss: 33.6214071624412 Accuracy: 0.8973684210526316\n",
      "Epoch: 53700 Loss: 33.60710153649816 Accuracy: 0.8973684210526316\n",
      "Epoch: 53720 Loss: 33.59291611495622 Accuracy: 0.8973684210526316\n",
      "Epoch: 53740 Loss: 33.578849945068185 Accuracy: 0.8973684210526316\n",
      "Epoch: 53760 Loss: 33.56490205549797 Accuracy: 0.8973684210526316\n",
      "Epoch: 53780 Loss: 33.55107145548518 Accuracy: 0.8973684210526316\n",
      "Epoch: 53800 Loss: 33.537357134446836 Accuracy: 0.8973684210526316\n",
      "Epoch: 53820 Loss: 33.523758061981205 Accuracy: 0.8973684210526316\n",
      "Epoch: 53840 Loss: 33.51027318823557 Accuracy: 0.8973684210526316\n",
      "Epoch: 53860 Loss: 33.496901444597846 Accuracy: 0.8973684210526316\n",
      "Epoch: 53880 Loss: 33.48364174467075 Accuracy: 0.8973684210526316\n",
      "Epoch: 53900 Loss: 33.47049298548698 Accuracy: 0.8973684210526316\n",
      "Epoch: 53920 Loss: 33.45745404892418 Accuracy: 0.8973684210526316\n",
      "Epoch: 53940 Loss: 33.444523803279864 Accuracy: 0.8973684210526316\n",
      "Epoch: 53960 Loss: 33.43170110496803 Accuracy: 0.8973684210526316\n",
      "Epoch: 53980 Loss: 33.41898480030142 Accuracy: 0.8973684210526316\n",
      "Epoch: 54000 Loss: 33.40637372732645 Accuracy: 0.8973684210526316\n",
      "Epoch: 54020 Loss: 33.39386671768031 Accuracy: 0.8973684210526316\n",
      "Epoch: 54040 Loss: 33.38146259844349 Accuracy: 0.8973684210526316\n",
      "Epoch: 54060 Loss: 33.36916019396371 Accuracy: 0.8973684210526316\n",
      "Epoch: 54080 Loss: 33.35695832763103 Accuracy: 0.8973684210526316\n",
      "Epoch: 54100 Loss: 33.344855823587125 Accuracy: 0.8973684210526316\n",
      "Epoch: 54120 Loss: 33.33285150835454 Accuracy: 0.8973684210526316\n",
      "Epoch: 54140 Loss: 33.32094421237515 Accuracy: 0.8973684210526316\n",
      "Epoch: 54160 Loss: 33.30913277144968 Accuracy: 0.8973684210526316\n",
      "Epoch: 54180 Loss: 33.29741602807252 Accuracy: 0.8973684210526316\n",
      "Epoch: 54200 Loss: 33.285792832658565 Accuracy: 0.8973684210526316\n",
      "Epoch: 54220 Loss: 33.27426204466087 Accuracy: 0.8973684210526316\n",
      "Epoch: 54240 Loss: 33.2628225335792 Accuracy: 0.8973684210526316\n",
      "Epoch: 54260 Loss: 33.25147317986175 Accuracy: 0.8973684210526316\n",
      "Epoch: 54280 Loss: 33.24021287570275 Accuracy: 0.8973684210526316\n",
      "Epoch: 54300 Loss: 33.22904052574026 Accuracy: 0.9\n",
      "Epoch: 54320 Loss: 33.217955047658776 Accuracy: 0.9\n",
      "Epoch: 54340 Loss: 33.20695537270228 Accuracy: 0.9\n",
      "Epoch: 54360 Loss: 33.19604044610314 Accuracy: 0.9\n",
      "Epoch: 54380 Loss: 33.185209227433 Accuracy: 0.9\n",
      "Epoch: 54400 Loss: 33.1744606908818 Accuracy: 0.9\n",
      "Epoch: 54420 Loss: 33.16379382547082 Accuracy: 0.9\n",
      "Epoch: 54440 Loss: 33.153207635205575 Accuracy: 0.9\n",
      "Epoch: 54460 Loss: 33.14270113917465 Accuracy: 0.9\n",
      "Epoch: 54480 Loss: 33.132273371599474 Accuracy: 0.9\n",
      "Epoch: 54500 Loss: 33.121923381840716 Accuracy: 0.9\n",
      "Epoch: 54520 Loss: 33.11165023436585 Accuracy: 0.9026315789473685\n",
      "Epoch: 54540 Loss: 33.10145300868266 Accuracy: 0.9026315789473685\n",
      "Epoch: 54560 Loss: 33.091330799242805 Accuracy: 0.9026315789473685\n",
      "Epoch: 54580 Loss: 33.08128271531942 Accuracy: 0.9026315789473685\n",
      "Epoch: 54600 Loss: 33.07130788086215 Accuracy: 0.9026315789473685\n",
      "Epoch: 54620 Loss: 33.06140543433318 Accuracy: 0.9026315789473685\n",
      "Epoch: 54640 Loss: 33.051574528526785 Accuracy: 0.9026315789473685\n",
      "Epoch: 54660 Loss: 33.04181433037529 Accuracy: 0.9026315789473685\n",
      "Epoch: 54680 Loss: 33.03212402074379 Accuracy: 0.9026315789473685\n",
      "Epoch: 54700 Loss: 33.022502794215654 Accuracy: 0.9026315789473685\n",
      "Epoch: 54720 Loss: 33.0129498588707 Accuracy: 0.9026315789473685\n",
      "Epoch: 54740 Loss: 33.00346443605768 Accuracy: 0.9026315789473685\n",
      "Epoch: 54760 Loss: 32.99404576016277 Accuracy: 0.9026315789473685\n",
      "Epoch: 54780 Loss: 32.98469307837479 Accuracy: 0.9026315789473685\n",
      "Epoch: 54800 Loss: 32.975405650448934 Accuracy: 0.9026315789473685\n",
      "Epoch: 54820 Loss: 32.96618274846956 Accuracy: 0.9026315789473685\n",
      "Epoch: 54840 Loss: 32.95702365661289 Accuracy: 0.9026315789473685\n",
      "Epoch: 54860 Loss: 32.94792767091062 Accuracy: 0.9026315789473685\n",
      "Epoch: 54880 Loss: 32.93889409901483 Accuracy: 0.9026315789473685\n",
      "Epoch: 54900 Loss: 32.92992225996466 Accuracy: 0.9026315789473685\n",
      "Epoch: 54920 Loss: 32.92101148395561 Accuracy: 0.9026315789473685\n",
      "Epoch: 54940 Loss: 32.91216111211126 Accuracy: 0.9026315789473685\n",
      "Epoch: 54960 Loss: 32.9033704962583 Accuracy: 0.9026315789473685\n",
      "Epoch: 54980 Loss: 32.89463899870471 Accuracy: 0.9026315789473685\n",
      "Epoch: 55000 Loss: 32.885965992021525 Accuracy: 0.9026315789473685\n",
      "Epoch: 55020 Loss: 32.87735085882836 Accuracy: 0.9026315789473685\n",
      "Epoch: 55040 Loss: 32.86879299158264 Accuracy: 0.9026315789473685\n",
      "Epoch: 55060 Loss: 32.860291792372976 Accuracy: 0.9026315789473685\n",
      "Epoch: 55080 Loss: 32.85184667271639 Accuracy: 0.9026315789473685\n",
      "Epoch: 55100 Loss: 32.843457053359735 Accuracy: 0.9026315789473685\n",
      "Epoch: 55120 Loss: 32.83512236408526 Accuracy: 0.9026315789473685\n",
      "Epoch: 55140 Loss: 32.82684204352016 Accuracy: 0.9026315789473685\n",
      "Epoch: 55160 Loss: 32.81861553895049 Accuracy: 0.9026315789473685\n",
      "Epoch: 55180 Loss: 32.81044230613901 Accuracy: 0.9026315789473685\n",
      "Epoch: 55200 Loss: 32.80232180914727 Accuracy: 0.9026315789473685\n",
      "Epoch: 55220 Loss: 32.7942535201617 Accuracy: 0.9026315789473685\n",
      "Epoch: 55240 Loss: 32.78623691932374 Accuracy: 0.9026315789473685\n",
      "Epoch: 55260 Loss: 32.778271494563995 Accuracy: 0.9026315789473685\n",
      "Epoch: 55280 Loss: 32.77035674144025 Accuracy: 0.9026315789473685\n",
      "Epoch: 55300 Loss: 32.76249216297941 Accuracy: 0.9026315789473685\n",
      "Epoch: 55320 Loss: 32.75467726952327 Accuracy: 0.9026315789473685\n",
      "Epoch: 55340 Loss: 32.746911578577894 Accuracy: 0.9026315789473685\n",
      "Epoch: 55360 Loss: 32.7391946146668 Accuracy: 0.9026315789473685\n",
      "Epoch: 55380 Loss: 32.73152590918773 Accuracy: 0.9026315789473685\n",
      "Epoch: 55400 Loss: 32.723905000272836 Accuracy: 0.9026315789473685\n",
      "Epoch: 55420 Loss: 32.71633143265241 Accuracy: 0.9026315789473685\n",
      "Epoch: 55440 Loss: 32.70880475752196 Accuracy: 0.9026315789473685\n",
      "Epoch: 55460 Loss: 32.70132453241256 Accuracy: 0.9026315789473685\n",
      "Epoch: 55480 Loss: 32.693890321064444 Accuracy: 0.9026315789473685\n",
      "Epoch: 55500 Loss: 32.68650169330375 Accuracy: 0.9026315789473685\n",
      "Epoch: 55520 Loss: 32.67915822492225 Accuracy: 0.9026315789473685\n",
      "Epoch: 55540 Loss: 32.671859497560206 Accuracy: 0.9026315789473685\n",
      "Epoch: 55560 Loss: 32.66460509859199 Accuracy: 0.9026315789473685\n",
      "Epoch: 55580 Loss: 32.657394621014674 Accuracy: 0.9026315789473685\n",
      "Epoch: 55600 Loss: 32.65022766333925 Accuracy: 0.9026315789473685\n",
      "Epoch: 55620 Loss: 32.64310382948471 Accuracy: 0.9026315789473685\n",
      "Epoch: 55640 Loss: 32.636022728674526 Accuracy: 0.9026315789473685\n",
      "Epoch: 55660 Loss: 32.628983975335956 Accuracy: 0.9026315789473685\n",
      "Epoch: 55680 Loss: 32.62198718900155 Accuracy: 0.9026315789473685\n",
      "Epoch: 55700 Loss: 32.61503199421328 Accuracy: 0.9026315789473685\n",
      "Epoch: 55720 Loss: 32.60811802042893 Accuracy: 0.9026315789473685\n",
      "Epoch: 55740 Loss: 32.601244901930755 Accuracy: 0.9026315789473685\n",
      "Epoch: 55760 Loss: 32.59441227773638 Accuracy: 0.9026315789473685\n",
      "Epoch: 55780 Loss: 32.58761979151192 Accuracy: 0.9026315789473685\n",
      "Epoch: 55800 Loss: 32.58086709148705 Accuracy: 0.9026315789473685\n",
      "Epoch: 55820 Loss: 32.57415383037231 Accuracy: 0.9026315789473685\n",
      "Epoch: 55840 Loss: 32.56747966527819 Accuracy: 0.9026315789473685\n",
      "Epoch: 55860 Loss: 32.560844257636305 Accuracy: 0.9026315789473685\n",
      "Epoch: 55880 Loss: 32.55424727312233 Accuracy: 0.9026315789473685\n",
      "Epoch: 55900 Loss: 32.54768838158084 Accuracy: 0.9026315789473685\n",
      "Epoch: 55920 Loss: 32.54116725695187 Accuracy: 0.9026315789473685\n",
      "Epoch: 55940 Loss: 32.534683577199175 Accuracy: 0.9026315789473685\n",
      "Epoch: 55960 Loss: 32.52823702424023 Accuracy: 0.9026315789473685\n",
      "Epoch: 55980 Loss: 32.521827283877755 Accuracy: 0.9026315789473685\n",
      "Epoch: 56000 Loss: 32.51545404573294 Accuracy: 0.9026315789473685\n",
      "Epoch: 56020 Loss: 32.509117003180094 Accuracy: 0.9026315789473685\n",
      "Epoch: 56040 Loss: 32.50281585328285 Accuracy: 0.9026315789473685\n",
      "Epoch: 56060 Loss: 32.496550296731755 Accuracy: 0.9026315789473685\n",
      "Epoch: 56080 Loss: 32.49032003778337 Accuracy: 0.9026315789473685\n",
      "Epoch: 56100 Loss: 32.48412478420063 Accuracy: 0.9026315789473685\n",
      "Epoch: 56120 Loss: 32.47796424719461 Accuracy: 0.9026315789473685\n",
      "Epoch: 56140 Loss: 32.47183814136754 Accuracy: 0.9026315789473685\n",
      "Epoch: 56160 Loss: 32.465746184657114 Accuracy: 0.9026315789473685\n",
      "Epoch: 56180 Loss: 32.45968809828199 Accuracy: 0.9026315789473685\n",
      "Epoch: 56200 Loss: 32.453663606688515 Accuracy: 0.9026315789473685\n",
      "Epoch: 56220 Loss: 32.44767243749857 Accuracy: 0.9026315789473685\n",
      "Epoch: 56240 Loss: 32.44171432145856 Accuracy: 0.9026315789473685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56260 Loss: 32.43578899238955 Accuracy: 0.9026315789473685\n",
      "Epoch: 56280 Loss: 32.42989618713833 Accuracy: 0.9026315789473685\n",
      "Epoch: 56300 Loss: 32.42403564552966 Accuracy: 0.9026315789473685\n",
      "Epoch: 56320 Loss: 32.41820711031951 Accuracy: 0.9026315789473685\n",
      "Epoch: 56340 Loss: 32.41241032714913 Accuracy: 0.9026315789473685\n",
      "Epoch: 56360 Loss: 32.406645044500294 Accuracy: 0.9026315789473685\n",
      "Epoch: 56380 Loss: 32.400911013651324 Accuracy: 0.9026315789473685\n",
      "Epoch: 56400 Loss: 32.3952079886341 Accuracy: 0.9026315789473685\n",
      "Epoch: 56420 Loss: 32.38953572619185 Accuracy: 0.9026315789473685\n",
      "Epoch: 56440 Loss: 32.38389398573798 Accuracy: 0.9026315789473685\n",
      "Epoch: 56460 Loss: 32.37828252931553 Accuracy: 0.9026315789473685\n",
      "Epoch: 56480 Loss: 32.37270112155763 Accuracy: 0.9026315789473685\n",
      "Epoch: 56500 Loss: 32.36714952964863 Accuracy: 0.9026315789473685\n",
      "Epoch: 56520 Loss: 32.361627523286046 Accuracy: 0.9026315789473685\n",
      "Epoch: 56540 Loss: 32.3561348746433 Accuracy: 0.9026315789473685\n",
      "Epoch: 56560 Loss: 32.35067135833311 Accuracy: 0.9026315789473685\n",
      "Epoch: 56580 Loss: 32.345236751371694 Accuracy: 0.9026315789473685\n",
      "Epoch: 56600 Loss: 32.33983083314358 Accuracy: 0.9026315789473685\n",
      "Epoch: 56620 Loss: 32.3344533853672 Accuracy: 0.9026315789473685\n",
      "Epoch: 56640 Loss: 32.329104192061045 Accuracy: 0.9026315789473685\n",
      "Epoch: 56660 Loss: 32.32378303951054 Accuracy: 0.9026315789473685\n",
      "Epoch: 56680 Loss: 32.31848971623553 Accuracy: 0.9026315789473685\n",
      "Epoch: 56700 Loss: 32.3132240129584 Accuracy: 0.9026315789473685\n",
      "Epoch: 56720 Loss: 32.307985722572745 Accuracy: 0.9026315789473685\n",
      "Epoch: 56740 Loss: 32.30277464011271 Accuracy: 0.9026315789473685\n",
      "Epoch: 56760 Loss: 32.29759056272281 Accuracy: 0.9026315789473685\n",
      "Epoch: 56780 Loss: 32.292433289628406 Accuracy: 0.9026315789473685\n",
      "Epoch: 56800 Loss: 32.287302622106665 Accuracy: 0.9026315789473685\n",
      "Epoch: 56820 Loss: 32.28219836345803 Accuracy: 0.9026315789473685\n",
      "Epoch: 56840 Loss: 32.277120318978334 Accuracy: 0.9026315789473685\n",
      "Epoch: 56860 Loss: 32.272068295931255 Accuracy: 0.9026315789473685\n",
      "Epoch: 56880 Loss: 32.26704210352142 Accuracy: 0.9026315789473685\n",
      "Epoch: 56900 Loss: 32.26204155286787 Accuracy: 0.9026315789473685\n",
      "Epoch: 56920 Loss: 32.257066456978116 Accuracy: 0.9026315789473685\n",
      "Epoch: 56940 Loss: 32.25211663072255 Accuracy: 0.9026315789473685\n",
      "Epoch: 56960 Loss: 32.24719189080939 Accuracy: 0.9026315789473685\n",
      "Epoch: 56980 Loss: 32.242292055760004 Accuracy: 0.9026315789473685\n",
      "Epoch: 57000 Loss: 32.237416945884725 Accuracy: 0.9026315789473685\n",
      "Epoch: 57020 Loss: 32.23256638325908 Accuracy: 0.9026315789473685\n",
      "Epoch: 57040 Loss: 32.227740191700356 Accuracy: 0.9026315789473685\n",
      "Epoch: 57060 Loss: 32.22293819674468 Accuracy: 0.9026315789473685\n",
      "Epoch: 57080 Loss: 32.21816022562442 Accuracy: 0.9026315789473685\n",
      "Epoch: 57100 Loss: 32.213406107246016 Accuracy: 0.9026315789473685\n",
      "Epoch: 57120 Loss: 32.2086756721681 Accuracy: 0.9026315789473685\n",
      "Epoch: 57140 Loss: 32.20396875258014 Accuracy: 0.9026315789473685\n",
      "Epoch: 57160 Loss: 32.199285182281336 Accuracy: 0.9026315789473685\n",
      "Epoch: 57180 Loss: 32.19462479665984 Accuracy: 0.9026315789473685\n",
      "Epoch: 57200 Loss: 32.18998743267242 Accuracy: 0.9026315789473685\n",
      "Epoch: 57220 Loss: 32.185372928824414 Accuracy: 0.9026315789473685\n",
      "Epoch: 57240 Loss: 32.180781125150006 Accuracy: 0.9026315789473685\n",
      "Epoch: 57260 Loss: 32.176211863192826 Accuracy: 0.9026315789473685\n",
      "Epoch: 57280 Loss: 32.17166498598692 Accuracy: 0.9026315789473685\n",
      "Epoch: 57300 Loss: 32.16714033803795 Accuracy: 0.9026315789473685\n",
      "Epoch: 57320 Loss: 32.16263776530475 Accuracy: 0.9026315789473685\n",
      "Epoch: 57340 Loss: 32.15815711518117 Accuracy: 0.9026315789473685\n",
      "Epoch: 57360 Loss: 32.153698236478206 Accuracy: 0.9026315789473685\n",
      "Epoch: 57380 Loss: 32.149260979406435 Accuracy: 0.9026315789473685\n",
      "Epoch: 57400 Loss: 32.14484519555866 Accuracy: 0.9026315789473685\n",
      "Epoch: 57420 Loss: 32.14045073789296 Accuracy: 0.9026315789473685\n",
      "Epoch: 57440 Loss: 32.13607746071588 Accuracy: 0.9026315789473685\n",
      "Epoch: 57460 Loss: 32.13172521966594 Accuracy: 0.9026315789473685\n",
      "Epoch: 57480 Loss: 32.12739387169739 Accuracy: 0.9026315789473685\n",
      "Epoch: 57500 Loss: 32.123083275064275 Accuracy: 0.9026315789473685\n",
      "Epoch: 57520 Loss: 32.118793289304605 Accuracy: 0.9026315789473685\n",
      "Epoch: 57540 Loss: 32.114523775224924 Accuracy: 0.9026315789473685\n",
      "Epoch: 57560 Loss: 32.11027459488506 Accuracy: 0.9026315789473685\n",
      "Epoch: 57580 Loss: 32.10604561158305 Accuracy: 0.9026315789473685\n",
      "Epoch: 57600 Loss: 32.10183668984034 Accuracy: 0.9026315789473685\n",
      "Epoch: 57620 Loss: 32.09764769538728 Accuracy: 0.9026315789473685\n",
      "Epoch: 57640 Loss: 32.09347849514869 Accuracy: 0.9026315789473685\n",
      "Epoch: 57660 Loss: 32.08932895722978 Accuracy: 0.9026315789473685\n",
      "Epoch: 57680 Loss: 32.085198950902154 Accuracy: 0.9026315789473685\n",
      "Epoch: 57700 Loss: 32.08108834659016 Accuracy: 0.9026315789473685\n",
      "Epoch: 57720 Loss: 32.07699701585731 Accuracy: 0.9026315789473685\n",
      "Epoch: 57740 Loss: 32.07292483139301 Accuracy: 0.9026315789473685\n",
      "Epoch: 57760 Loss: 32.0688716669994 Accuracy: 0.9026315789473685\n",
      "Epoch: 57780 Loss: 32.064837397578415 Accuracy: 0.9026315789473685\n",
      "Epoch: 57800 Loss: 32.0608218991191 Accuracy: 0.9026315789473685\n",
      "Epoch: 57820 Loss: 32.056825048684935 Accuracy: 0.9026315789473685\n",
      "Epoch: 57840 Loss: 32.052846724401576 Accuracy: 0.9026315789473685\n",
      "Epoch: 57860 Loss: 32.048886805444596 Accuracy: 0.9026315789473685\n",
      "Epoch: 57880 Loss: 32.04494517202744 Accuracy: 0.9026315789473685\n",
      "Epoch: 57900 Loss: 32.04102170538957 Accuracy: 0.9026315789473685\n",
      "Epoch: 57920 Loss: 32.03711628778484 Accuracy: 0.9026315789473685\n",
      "Epoch: 57940 Loss: 32.03322880246988 Accuracy: 0.9026315789473685\n",
      "Epoch: 57960 Loss: 32.029359133692786 Accuracy: 0.9026315789473685\n",
      "Epoch: 57980 Loss: 32.025507166681926 Accuracy: 0.9026315789473685\n",
      "Epoch: 58000 Loss: 32.02167278763483 Accuracy: 0.9026315789473685\n",
      "Epoch: 58020 Loss: 32.01785588370737 Accuracy: 0.9026315789473685\n",
      "Epoch: 58040 Loss: 32.01405634300295 Accuracy: 0.9026315789473685\n",
      "Epoch: 58060 Loss: 32.01027405456197 Accuracy: 0.9026315789473685\n",
      "Epoch: 58080 Loss: 32.00650890835132 Accuracy: 0.9026315789473685\n",
      "Epoch: 58100 Loss: 32.002760795254076 Accuracy: 0.9026315789473685\n",
      "Epoch: 58120 Loss: 31.999029607059384 Accuracy: 0.9026315789473685\n",
      "Epoch: 58140 Loss: 31.99531523645237 Accuracy: 0.9026315789473685\n",
      "Epoch: 58160 Loss: 31.991617577004234 Accuracy: 0.9026315789473685\n",
      "Epoch: 58180 Loss: 31.987936523162574 Accuracy: 0.9026315789473685\n",
      "Epoch: 58200 Loss: 31.984271970241615 Accuracy: 0.9026315789473685\n",
      "Epoch: 58220 Loss: 31.980623814412844 Accuracy: 0.9026315789473685\n",
      "Epoch: 58240 Loss: 31.976991952695517 Accuracy: 0.9026315789473685\n",
      "Epoch: 58260 Loss: 31.973376282947466 Accuracy: 0.9026315789473685\n",
      "Epoch: 58280 Loss: 31.96977670385595 Accuracy: 0.9026315789473685\n",
      "Epoch: 58300 Loss: 31.966193114928615 Accuracy: 0.9026315789473685\n",
      "Epoch: 58320 Loss: 31.96262541648462 Accuracy: 0.9026315789473685\n",
      "Epoch: 58340 Loss: 31.959073509645854 Accuracy: 0.9026315789473685\n",
      "Epoch: 58360 Loss: 31.955537296328252 Accuracy: 0.9026315789473685\n",
      "Epoch: 58380 Loss: 31.952016679233253 Accuracy: 0.9026315789473685\n",
      "Epoch: 58400 Loss: 31.948511561839318 Accuracy: 0.9026315789473685\n",
      "Epoch: 58420 Loss: 31.945021848393644 Accuracy: 0.9026315789473685\n",
      "Epoch: 58440 Loss: 31.94154744390387 Accuracy: 0.9026315789473685\n",
      "Epoch: 58460 Loss: 31.938088254129987 Accuracy: 0.9026315789473685\n",
      "Epoch: 58480 Loss: 31.934644185576275 Accuracy: 0.9026315789473685\n",
      "Epoch: 58500 Loss: 31.931215145483428 Accuracy: 0.9026315789473685\n",
      "Epoch: 58520 Loss: 31.927801041820658 Accuracy: 0.9026315789473685\n",
      "Epoch: 58540 Loss: 31.92440178327802 Accuracy: 0.9026315789473685\n",
      "Epoch: 58560 Loss: 31.921017279258727 Accuracy: 0.9026315789473685\n",
      "Epoch: 58580 Loss: 31.917647439871665 Accuracy: 0.9026315789473685\n",
      "Epoch: 58600 Loss: 31.914292175923883 Accuracy: 0.9026315789473685\n",
      "Epoch: 58620 Loss: 31.91095139891329 Accuracy: 0.9026315789473685\n",
      "Epoch: 58640 Loss: 31.90762502102133 Accuracy: 0.9026315789473685\n",
      "Epoch: 58660 Loss: 31.90431295510586 Accuracy: 0.9026315789473685\n",
      "Epoch: 58680 Loss: 31.90101511469402 Accuracy: 0.9026315789473685\n",
      "Epoch: 58700 Loss: 31.897731413975222 Accuracy: 0.9026315789473685\n",
      "Epoch: 58720 Loss: 31.894461767794265 Accuracy: 0.9026315789473685\n",
      "Epoch: 58740 Loss: 31.89120609164442 Accuracy: 0.9026315789473685\n",
      "Epoch: 58760 Loss: 31.887964301660755 Accuracy: 0.9026315789473685\n",
      "Epoch: 58780 Loss: 31.884736314613395 Accuracy: 0.9026315789473685\n",
      "Epoch: 58800 Loss: 31.881522047900944 Accuracy: 0.9026315789473685\n",
      "Epoch: 58820 Loss: 31.87832141954395 Accuracy: 0.9026315789473685\n",
      "Epoch: 58840 Loss: 31.875134348178456 Accuracy: 0.9026315789473685\n",
      "Epoch: 58860 Loss: 31.871960753049635 Accuracy: 0.9026315789473685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58880 Loss: 31.868800554005503 Accuracy: 0.9026315789473685\n",
      "Epoch: 58900 Loss: 31.865653671490648 Accuracy: 0.9026315789473685\n",
      "Epoch: 58920 Loss: 31.86252002654011 Accuracy: 0.9026315789473685\n",
      "Epoch: 58940 Loss: 31.8593995407733 Accuracy: 0.9026315789473685\n",
      "Epoch: 58960 Loss: 31.856292136387935 Accuracy: 0.9026315789473685\n",
      "Epoch: 58980 Loss: 31.85319773615416 Accuracy: 0.9026315789473685\n",
      "Epoch: 59000 Loss: 31.8501162634086 Accuracy: 0.9026315789473685\n",
      "Epoch: 59020 Loss: 31.84704764204858 Accuracy: 0.9026315789473685\n",
      "Epoch: 59040 Loss: 31.84399179652638 Accuracy: 0.9026315789473685\n",
      "Epoch: 59060 Loss: 31.840948651843515 Accuracy: 0.9026315789473685\n",
      "Epoch: 59080 Loss: 31.837918133545138 Accuracy: 0.9026315789473685\n",
      "Epoch: 59100 Loss: 31.834900167714483 Accuracy: 0.9026315789473685\n",
      "Epoch: 59120 Loss: 31.831894680967306 Accuracy: 0.9026315789473685\n",
      "Epoch: 59140 Loss: 31.828901600446546 Accuracy: 0.9026315789473685\n",
      "Epoch: 59160 Loss: 31.825920853816832 Accuracy: 0.9026315789473685\n",
      "Epoch: 59180 Loss: 31.822952369259234 Accuracy: 0.9026315789473685\n",
      "Epoch: 59200 Loss: 31.81999607546595 Accuracy: 0.9026315789473685\n",
      "Epoch: 59220 Loss: 31.81705190163516 Accuracy: 0.9026315789473685\n",
      "Epoch: 59240 Loss: 31.81411977746575 Accuracy: 0.9026315789473685\n",
      "Epoch: 59260 Loss: 31.811199633152334 Accuracy: 0.9026315789473685\n",
      "Epoch: 59280 Loss: 31.808291399380156 Accuracy: 0.9026315789473685\n",
      "Epoch: 59300 Loss: 31.80539500732007 Accuracy: 0.9026315789473685\n",
      "Epoch: 59320 Loss: 31.80251038862363 Accuracy: 0.9026315789473685\n",
      "Epoch: 59340 Loss: 31.799637475418194 Accuracy: 0.9026315789473685\n",
      "Epoch: 59360 Loss: 31.796776200302066 Accuracy: 0.9026315789473685\n",
      "Epoch: 59380 Loss: 31.79392649633971 Accuracy: 0.9026315789473685\n",
      "Epoch: 59400 Loss: 31.791088297057037 Accuracy: 0.9026315789473685\n",
      "Epoch: 59420 Loss: 31.788261536436643 Accuracy: 0.9026315789473685\n",
      "Epoch: 59440 Loss: 31.78544614891321 Accuracy: 0.9026315789473685\n",
      "Epoch: 59460 Loss: 31.78264206936891 Accuracy: 0.9026315789473685\n",
      "Epoch: 59480 Loss: 31.779849233128814 Accuracy: 0.9026315789473685\n",
      "Epoch: 59500 Loss: 31.777067575956387 Accuracy: 0.9026315789473685\n",
      "Epoch: 59520 Loss: 31.774297034049063 Accuracy: 0.9026315789473685\n",
      "Epoch: 59540 Loss: 31.771537544033762 Accuracy: 0.9026315789473685\n",
      "Epoch: 59560 Loss: 31.768789042962553 Accuracy: 0.9026315789473685\n",
      "Epoch: 59580 Loss: 31.766051468308277 Accuracy: 0.9026315789473685\n",
      "Epoch: 59600 Loss: 31.763324757960312 Accuracy: 0.9026315789473685\n",
      "Epoch: 59620 Loss: 31.760608850220237 Accuracy: 0.9026315789473685\n",
      "Epoch: 59640 Loss: 31.75790368379768 Accuracy: 0.9026315789473685\n",
      "Epoch: 59660 Loss: 31.75520919780611 Accuracy: 0.9026315789473685\n",
      "Epoch: 59680 Loss: 31.752525331758722 Accuracy: 0.9026315789473685\n",
      "Epoch: 59700 Loss: 31.749852025564305 Accuracy: 0.9026315789473685\n",
      "Epoch: 59720 Loss: 31.747189219523214 Accuracy: 0.9026315789473685\n",
      "Epoch: 59740 Loss: 31.74453685432332 Accuracy: 0.9026315789473685\n",
      "Epoch: 59760 Loss: 31.741894871036067 Accuracy: 0.9026315789473685\n",
      "Epoch: 59780 Loss: 31.739263211112444 Accuracy: 0.9026315789473685\n",
      "Epoch: 59800 Loss: 31.736641816379144 Accuracy: 0.9026315789473685\n",
      "Epoch: 59820 Loss: 31.73403062903465 Accuracy: 0.9026315789473685\n",
      "Epoch: 59840 Loss: 31.731429591645373 Accuracy: 0.9026315789473685\n",
      "Epoch: 59860 Loss: 31.72883864714187 Accuracy: 0.9026315789473685\n",
      "Epoch: 59880 Loss: 31.726257738815047 Accuracy: 0.9026315789473685\n",
      "Epoch: 59900 Loss: 31.7236868103124 Accuracy: 0.9026315789473685\n",
      "Epoch: 59920 Loss: 31.721125805634312 Accuracy: 0.9026315789473685\n",
      "Epoch: 59940 Loss: 31.718574669130387 Accuracy: 0.9026315789473685\n",
      "Epoch: 59960 Loss: 31.716033345495752 Accuracy: 0.9026315789473685\n",
      "Epoch: 59980 Loss: 31.713501779767455 Accuracy: 0.9026315789473685\n",
      "Epoch: 60000 Loss: 31.710979917320905 Accuracy: 0.9026315789473685\n",
      "Epoch: 60020 Loss: 31.708467703866233 Accuracy: 0.9026315789473685\n",
      "Epoch: 60040 Loss: 31.70596508544483 Accuracy: 0.9026315789473685\n",
      "Epoch: 60060 Loss: 31.703472008425802 Accuracy: 0.9026315789473685\n",
      "Epoch: 60080 Loss: 31.700988419502494 Accuracy: 0.9026315789473685\n",
      "Epoch: 60100 Loss: 31.69851426568902 Accuracy: 0.9026315789473685\n",
      "Epoch: 60120 Loss: 31.696049494316902 Accuracy: 0.9026315789473685\n",
      "Epoch: 60140 Loss: 31.693594053031603 Accuracy: 0.9026315789473685\n",
      "Epoch: 60160 Loss: 31.69114788978918 Accuracy: 0.9026315789473685\n",
      "Epoch: 60180 Loss: 31.688710952852958 Accuracy: 0.9026315789473685\n",
      "Epoch: 60200 Loss: 31.686283190790135 Accuracy: 0.9026315789473685\n",
      "Epoch: 60220 Loss: 31.68386455246859 Accuracy: 0.9026315789473685\n",
      "Epoch: 60240 Loss: 31.68145498705354 Accuracy: 0.9026315789473685\n",
      "Epoch: 60260 Loss: 31.679054444004276 Accuracy: 0.9026315789473685\n",
      "Epoch: 60280 Loss: 31.676662873071013 Accuracy: 0.9026315789473685\n",
      "Epoch: 60300 Loss: 31.6742802242916 Accuracy: 0.9026315789473685\n",
      "Epoch: 60320 Loss: 31.671906447988388 Accuracy: 0.9026315789473685\n",
      "Epoch: 60340 Loss: 31.669541494765078 Accuracy: 0.9026315789473685\n",
      "Epoch: 60360 Loss: 31.667185315503545 Accuracy: 0.9026315789473685\n",
      "Epoch: 60380 Loss: 31.66483786136073 Accuracy: 0.9026315789473685\n",
      "Epoch: 60400 Loss: 31.6624990837656 Accuracy: 0.9026315789473685\n",
      "Epoch: 60420 Loss: 31.66016893441597 Accuracy: 0.9026315789473685\n",
      "Epoch: 60440 Loss: 31.657847365275565 Accuracy: 0.9026315789473685\n",
      "Epoch: 60460 Loss: 31.65553432857088 Accuracy: 0.9026315789473685\n",
      "Epoch: 60480 Loss: 31.653229776788223 Accuracy: 0.9026315789473685\n",
      "Epoch: 60500 Loss: 31.650933662670695 Accuracy: 0.9026315789473685\n",
      "Epoch: 60520 Loss: 31.648645939215232 Accuracy: 0.9026315789473685\n",
      "Epoch: 60540 Loss: 31.646366559669605 Accuracy: 0.9026315789473685\n",
      "Epoch: 60560 Loss: 31.644095477529525 Accuracy: 0.9026315789473685\n",
      "Epoch: 60580 Loss: 31.641832646535672 Accuracy: 0.9026315789473685\n",
      "Epoch: 60600 Loss: 31.639578020670807 Accuracy: 0.9026315789473685\n",
      "Epoch: 60620 Loss: 31.637331554156873 Accuracy: 0.9026315789473685\n",
      "Epoch: 60640 Loss: 31.635093201452136 Accuracy: 0.9026315789473685\n",
      "Epoch: 60660 Loss: 31.632862917248303 Accuracy: 0.9026315789473685\n",
      "Epoch: 60680 Loss: 31.630640656467662 Accuracy: 0.9026315789473685\n",
      "Epoch: 60700 Loss: 31.628426374260297 Accuracy: 0.9026315789473685\n",
      "Epoch: 60720 Loss: 31.62622002600125 Accuracy: 0.9026315789473685\n",
      "Epoch: 60740 Loss: 31.6240215672877 Accuracy: 0.9\n",
      "Epoch: 60760 Loss: 31.621830953936204 Accuracy: 0.9\n",
      "Epoch: 60780 Loss: 31.619648141979923 Accuracy: 0.9\n",
      "Epoch: 60800 Loss: 31.61747308766585 Accuracy: 0.9\n",
      "Epoch: 60820 Loss: 31.615305747452084 Accuracy: 0.9\n",
      "Epoch: 60840 Loss: 31.61314607800507 Accuracy: 0.9\n",
      "Epoch: 60860 Loss: 31.6109940361969 Accuracy: 0.9\n",
      "Epoch: 60880 Loss: 31.608849579102593 Accuracy: 0.9\n",
      "Epoch: 60900 Loss: 31.606712663997428 Accuracy: 0.9\n",
      "Epoch: 60920 Loss: 31.604583248354228 Accuracy: 0.9\n",
      "Epoch: 60940 Loss: 31.60246128984069 Accuracy: 0.9\n",
      "Epoch: 60960 Loss: 31.600346746316774 Accuracy: 0.9\n",
      "Epoch: 60980 Loss: 31.598239575831954 Accuracy: 0.9\n",
      "Epoch: 61000 Loss: 31.596139736622714 Accuracy: 0.9\n",
      "Epoch: 61020 Loss: 31.59404718710979 Accuracy: 0.9\n",
      "Epoch: 61040 Loss: 31.59196188589565 Accuracy: 0.9\n",
      "Epoch: 61060 Loss: 31.589883791761828 Accuracy: 0.9\n",
      "Epoch: 61080 Loss: 31.587812863666343 Accuracy: 0.9\n",
      "Epoch: 61100 Loss: 31.58574906074115 Accuracy: 0.9\n",
      "Epoch: 61120 Loss: 31.58369234228948 Accuracy: 0.9\n",
      "Epoch: 61140 Loss: 31.58164266778335 Accuracy: 0.9\n",
      "Epoch: 61160 Loss: 31.57959999686095 Accuracy: 0.9\n",
      "Epoch: 61180 Loss: 31.57756428932415 Accuracy: 0.9\n",
      "Epoch: 61200 Loss: 31.575535505135885 Accuracy: 0.9\n",
      "Epoch: 61220 Loss: 31.573513604417656 Accuracy: 0.9\n",
      "Epoch: 61240 Loss: 31.571498547447018 Accuracy: 0.9\n",
      "Epoch: 61260 Loss: 31.569490294655047 Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "NN_obj.train(X=xtrain,Y=ytrain,learningRate=1,batchMode=True,batchSize=100,epochsToRun=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "0.85\n"
     ]
    }
   ],
   "source": [
    "print (NN_obj.getAccuracy(xtrain,ytrain))\n",
    "print (NN_obj.getAccuracy(xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
