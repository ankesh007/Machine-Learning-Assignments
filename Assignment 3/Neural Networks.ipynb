{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,inputNodes,hiddenList,hiddenActivations,outputNodes=1,outputActivation=\"SIGMOID\"):\n",
    "        self.inputNodes=inputNodes\n",
    "        self.outputNodes=outputNodes\n",
    "        self.hiddenList=[inputNodes]+hiddenList+[outputNodes]\n",
    "        # Nodes in each Layer (Also stores corresponding to input and output layer)\n",
    "        self.Activations=[\"None\"]+hiddenActivations+[outputActivation]\n",
    "        # Activation Functions (Also stores corresponding to input and output layer)\n",
    "        self.weight=[]\n",
    "        self.os=[]\n",
    "        self.net=[]\n",
    "        self.deltas=[]\n",
    "        self.loss=0\n",
    "        self.trainsteps=0\n",
    "        self.epoch=0\n",
    "    \n",
    "    # Initialise weight Matrices Randomly\n",
    "    def initialiseWeight(self):\n",
    "        layers=len(self.hiddenList)\n",
    "        \n",
    "        for i in range(1,layers,1):\n",
    "            self.weight.append(np.random.randn(self.hiddenList[i],self.hiddenList[i-1]+1))\n",
    "    \n",
    "    # Applies Activation Function\n",
    "    def applyActivation(self,vector,activation=\"None\"):\n",
    "        if(activation==\"RELU\"):\n",
    "            return max(vector,0)\n",
    "        elif(activation==\"SIGMOID\"):\n",
    "            return (1.0/(1+np.exp(-vector)))\n",
    "        else:\n",
    "            return 1*vector\n",
    "    \n",
    "    def append1(self,x):\n",
    "        [instances,attr]=x.shape\n",
    "        bias=np.ones([instances,1])\n",
    "        return np.concatenate((bias,x),axis=1)\n",
    "    \n",
    "    # Forward Pass a batch through the network\n",
    "    def forwardPass(self,batchX):\n",
    "        \n",
    "        layers=len(self.weight)\n",
    "        self.os=[self.append1(batchX)]\n",
    "        self.net=[self.append1(batchX)]\n",
    "                \n",
    "        for i in range(layers):\n",
    "            x=self.os[i]\n",
    "            netj=np.matmul(x,self.weight[i].T)\n",
    "            o=(self.applyActivation(netj,self.Activations[i+1]))\n",
    "            if(i==layers-1):\n",
    "                self.net.append(netj)\n",
    "                self.os.append(o)\n",
    "            else:\n",
    "                self.net.append(self.append1(netj))\n",
    "                self.os.append(self.append1(o))\n",
    "    \n",
    "    # Euclidean Loss -> if Changed, gradient J wrt o must be updated\n",
    "    def lossfunction(self,pred,y):\n",
    "        return (pred-y)**2\n",
    "    \n",
    "    # Function returning loss on batch\n",
    "    def getLoss(self,batchX,batchY):\n",
    "        self.forwardPass(batchX)\n",
    "        layers=len(self.os)\n",
    "        finalOutput=self.os[layers-1]\n",
    "        return np.sum(self.lossfunction(finalOutput,batchY))\n",
    "\n",
    "    # Gradient of o wrt net \n",
    "    def gradient_o_net(self,o,net,activation=\"None\"):\n",
    "        if(activation==\"RELU\"):\n",
    "            return (net>=0)\n",
    "        elif(activation==\"SIGMOID\"):\n",
    "            return o*(1-o)\n",
    "        else:\n",
    "            return (o==o)\n",
    "    \n",
    "    def gradient_o_net_layer(self,layerNo):\n",
    "        return self.gradient_o_net(self.os[layerNo],self.net[layerNo],self.Activations[layerNo])\n",
    "    \n",
    "    # Gradient of J wrt o\n",
    "    def gradient_J_o(self,o,y):\n",
    "        return 2*(o-y)        \n",
    "\n",
    "    # Backward Pass a batch through the network updating Weight Matrices   \n",
    "    def backwardPass(self,batchX,batchY,learningRate=0.001):\n",
    "                \n",
    "        layers=len(self.os)\n",
    "        m=float(1)\n",
    "#         m=float((batchX.shape)[0])\n",
    "        self.deltas=[]\n",
    "        \n",
    "        finalDelta=self.gradient_J_o(self.os[layers-1],batchY)*self.gradient_o_net_layer(layers-1)        \n",
    "        self.deltas.append(finalDelta)\n",
    "        weightMatrixLen=len(self.weight)\n",
    "        \n",
    "        for i in range(layers-2,0,-1):\n",
    "            weightMatrixLen-=1\n",
    "            current_o_net=self.gradient_o_net_layer(i)\n",
    "            current_delta=np.matmul(self.deltas[0],self.weight[weightMatrixLen])\n",
    "            current_delta=current_delta*current_o_net\n",
    "            [_,attributes]=current_delta.shape\n",
    "            self.deltas=[current_delta[:,1:attributes]]+self.deltas\n",
    "        \n",
    "        layers=len(self.weight)\n",
    "         \n",
    "        for i in range(layers-1,-1,-1):\n",
    "            del_w=np.matmul(self.deltas[i].T,self.os[i])\n",
    "            self.weight[i]-=(learningRate/m)*del_w\n",
    "\n",
    "    # training the NN\n",
    "    def train(self,X,Y,learningRate=0.001,batchMode=False,batchSize=100,epsilon=0.00001):\n",
    "        \n",
    "        [instances,attributes]=X.shape\n",
    "        [_,outputs]=Y.shape\n",
    "        \n",
    "        if(batchMode==False):\n",
    "            batchSize=instances\n",
    "        \n",
    "        Trained=False\n",
    "        prevLoss=self.getLoss(X,Y)\n",
    "        \n",
    "        while(not Trained):\n",
    "            self.epoch+=1\n",
    "            \n",
    "            cur=0\n",
    "            while(cur<instances):\n",
    "                uplim=min(cur+batchSize,instances)\n",
    "                batchX=X[cur:uplim,0:attributes]\n",
    "                batchY=Y[cur:uplim,0:outputs]\n",
    "                cur=uplim\n",
    "                self.forwardPass(batchX)\n",
    "                self.backwardPass(batchX,batchY,learningRate)\n",
    "                self.trainsteps+=1\n",
    "            \n",
    "            curLoss=self.getLoss(X,Y)\n",
    "            \n",
    "            if(abs(curLoss-prevLoss)<epsilon):\n",
    "                Trained=True\n",
    "            \n",
    "            prevLoss=curLoss\n",
    "            if(self.epoch%100==0):\n",
    "                print (self.epoch,prevLoss)\n",
    "                print (self.getAccuracy(X,Y))\n",
    "        \n",
    "    def predict(self,X):\n",
    "        self.forwardPass(X)\n",
    "        layers=len(self.os)\n",
    "        return self.os[layers-1]\n",
    "    \n",
    "    def getAccuracy(self,X,Y):\n",
    "        pred=self.predict(X)\n",
    "        instances=float(X.shape[0])\n",
    "        \n",
    "        pred=(pred>=0.5).astype(int)\n",
    "        \n",
    "        return (np.sum(pred==Y))/instances\n",
    "        \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain=pd.read_csv(\"Dataset/NN/toy_data/toy_trainX.csv\",header=None,sep=',').values\n",
    "ytrain=pd.read_csv(\"Dataset/NN/toy_data/toy_trainY.csv\",header=None,sep=',').values\n",
    "xtest=pd.read_csv(\"Dataset/NN/toy_data/toy_testX.csv\",header=None,sep=',').values\n",
    "ytest=pd.read_csv(\"Dataset/NN/toy_data/toy_testY.csv\",header=None,sep=',').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(380, 2)\n",
      "(380, 1)\n",
      "(120, 2)\n",
      "(120, 1)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "print(xtest.shape)\n",
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN_obj=NeuralNetwork(xtrain.shape[1],[5,5],[\"SIGMOID\",\"SIGMOID\"],ytrain.shape[1])\n",
    "NN_obj=NeuralNetwork(xtrain.shape[1],[5,5],[\"RELU\",\"RELU\"],ytrain.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_obj.initialiseWeight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3)\n",
      "(5, 6)\n",
      "(1, 6)\n"
     ]
    }
   ],
   "source": [
    "for weights in NN_obj.weight:\n",
    "    print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-4b360f526194>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNN_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-112-098698c97898>\u001b[0m in \u001b[0;36mgetLoss\u001b[0;34m(self, batchX, batchY)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# Function returning loss on batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardPass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mfinalOutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-112-098698c97898>\u001b[0m in \u001b[0;36mforwardPass\u001b[0;34m(self, batchX)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mnetj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplyActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-112-098698c97898>\u001b[0m in \u001b[0;36mapplyActivation\u001b[0;34m(self, vector, activation)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplyActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"RELU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"SIGMOID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "NN_obj.getLoss(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-d4e18983a37e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNN_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-112-098698c97898>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, learningRate, batchMode, batchSize, epsilon)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mTrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mprevLoss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mTrained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-112-098698c97898>\u001b[0m in \u001b[0;36mgetLoss\u001b[0;34m(self, batchX, batchY)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# Function returning loss on batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardPass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mfinalOutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-112-098698c97898>\u001b[0m in \u001b[0;36mforwardPass\u001b[0;34m(self, batchX)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mnetj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplyActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-112-098698c97898>\u001b[0m in \u001b[0;36mapplyActivation\u001b[0;34m(self, vector, activation)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplyActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"RELU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"SIGMOID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "NN_obj.train(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8973684210526316\n",
      "0.8583333333333333\n"
     ]
    }
   ],
   "source": [
    "print (NN_obj.getAccuracy(xtrain,ytrain))\n",
    "print (NN_obj.getAccuracy(xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
